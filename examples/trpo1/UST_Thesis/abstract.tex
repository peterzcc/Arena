%!TEX program = xelatex
%!TEX root = ./thesis.tex
\begin{center}
{\Large \thesistitle}\\
\vspace{20mm}
by \thesisauthor\\
\vspace{15mm}
\departmentname\\
\vspace{10mm}
The Hong Kong University of Science and Technology
\end{center}
\vspace{8mm}
\begin{center}
Abstract
\end{center}
One of the most important problems in artificial intelligence is learning to solve control problems without human supervision. Recent advances in end-to-end deep reinforcement learning methods have achieved significant progress in this domain, on a number of problems including a subset of Atari games~\cite{mnih2015human}, Go game~\cite{silver2016mastering}, and several simple robot control environments~\cite{duan2016benchmarking}.  However, a general solution to realistic robot control problems is still missing. Most of realistic robot control problems have multi-modal state space, which usually have both low-dimensional motion sensor input and high-dimensional image sensor input. Apart from that, a smooth and informative reward function is usually unavailable in realistic environments. Most realistic robot control problems only have sparse and discrete reward functions.

This thesis investigates several continuous control problems with the property of multi-modal state space and sparse reward functions. We propose several solutions for improving the performance of flat reinforcement learning methods on a subset of the proposed problems, including Wasserstein trust-region method, exceptional advantage regularization method, and robust concentric Gaussian mixture policy model. A hierarchical reinforcement learning method is also proposed to solve the target tasks without domain-specific knowledge given a pre-defined set of primary tasks. 

\par
\noindent


