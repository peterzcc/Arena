%!TEX program = xelatex
%!TEX root = ./thesis.tex
\section{Efficient Exploration Through Exceptional Advantage Regularization}
One of the major reasons that multi-modality robot environments are challenging is because the features in different modalities have different complexities.

In the target case, where there are two modality: the locomotion state vector and the image input, the agent is very likely to get stuck at a local minimum after having learned the features of the locomotion state vectors while haven't learned the image features. However, the policy is at a much lower entropy at the phase when the agent starts to learn image features.

As far as we know, the distribution type of the continuous stochastic policy in reinforcement learning is always chosen to be normal distribution with diagonal covariance matrices in the works on policy gradient methods . We have observed a phenomenon in the training process of policy gradient methods that, the variance of the policy is extremely unlikely to be increased, even when the agent has just escaped from a local minimum. Therefore, a regulation method becomes necessary to encourage the exploration of the agent when it finds that its policy has converged too fast at a sub-optimal point.

A conventional regularization method for encouraging exploration is entropy regularization:

\begin{align}
g' = g +\beta_{ent}\nabla_\theta \mathbb{E}[ H(\pi_\theta(a|s)) ]
\end{align}
Where $\beta_{ent}$ is the weight controlling the penalty on low entropies.
The entropy of a normal distribution $\mathcal{N}(\mu,\Sigma)$ is defined as:

\begin{align}
	H(\pi) =  \frac{1}{2} \ln \mathrm{det}(2\pi e \Sigma)
\end{align}

When the policy distribution is a normal distribution with diagonal covariance matrix, the entropy regularization basically introduces a constant bias in the gradient of the logarithm of variance parameters. As a result, the entropy regularization method is usually hard to tune, and leads to degradation on learning performance.

Here we propose a novel method that can efficiently encourage exploration without large penalty on the agent's learning performance. We add a loss, namely Exceptional Advantage Regularization loss (EAR), to the original gradient of the variance parameters:
\begin{align}
g'_\Sigma = g_\Sigma + \beta_{exc} \mathbb{E} \left[
	\max\left(0,I(\hat{A}^{GAE}>0) \hat{A}^{GAE} \nabla_\Sigma \log \pi (a|s)\right)\right]
\end{align}
where $\beta_{exc} $ is the weight controlling the bias on exploration, and $I(.)$ is the indicator function, which returns 1 if the expression is true, otherwise 0.

The EAR loss add weights for the positive gradients of the variance parameters of the samples with positive advantage values. The reason behind the loss is that we want to add more importance to the samples which produce exceptionally positive advantage value, but with low sampling likelihood. Therefore we first only focus on points with positive advantage values. The problem remaining is to identify the points with low likelihood, namely "exceptional" points. We simply choose these points that produce positive gradients any of the variance parameters.
