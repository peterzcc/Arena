\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{schulman2015high}
\citation{schulman2015high}
\citation{schulman2015high}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related works}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Policy Gradient Methods}{7}{section.2.1}}
\newlabel{pg_def}{{2.1}{7}{Policy Gradient Methods}{equation.2.1.1}{}}
\citation{schulman2015trust}
\citation{openaigym}
\citation{wu2017scalable}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Trust Region Policy Optimization}{8}{subsection.2.1.1}}
\newlabel{trpo_obj}{{2.2}{8}{Trust Region Policy Optimization}{equation.2.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}Kronecker-factored Trust Region}{8}{subsection.2.1.2}}
\citation{schulman2017proximal}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}Proximal Policy Gradient Method}{9}{subsection.2.1.3}}
\citation{lillicrap2015continuous}
\citation{wang2016sample}
\citation{nachum2017trust}
\citation{barto2003recent}
\citation{sutton1999between}
\citation{parr1998reinforcement}
\citation{dietterich2000hierarchical}
\citation{mcgovern2001automatic}
\citation{hengst2002discovering}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.4}Other Methods in Deep Reinforcement Learning for Continuous Control}{10}{subsection.2.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Hierarchical Reinforcement Learning Methods}{10}{section.2.2}}
\citation{sutton1999between}
\citation{sutton1999between}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Option framework}{11}{subsection.2.2.1}}
\citation{heess2016learning}
\citation{heess2016learning}
\citation{heess2016learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Modulated hierarchical controller}{12}{subsection.2.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces The hierarchical structure of the modulated controller\nobreakspace  {}\cite  {heess2016learning}, which consists of the recurrent high-level controller (HL, dark blue) and feedforward low-level controller (LL, green). The high-level controller has access to all observations (yellow and red). While both controllers observe sensory input at the world-clock frequency, the modulatory con- trol signal from the high-level is updated every K steps (here K = 2)}}{13}{figure.2.1}}
\newlabel{review_moduler_arch}{{2.1}{13}{The hierarchical structure of the modulated controller~\cite {heess2016learning}, which consists of the recurrent high-level controller (HL, dark blue) and feedforward low-level controller (LL, green). The high-level controller has access to all observations (yellow and red). While both controllers observe sensory input at the world-clock frequency, the modulatory con- trol signal from the high-level is updated every K steps (here K = 2)}{figure.2.1}{}}
\citation{frans2017meta}
\citation{frans2017meta}
\citation{frans2017meta}
\citation{duan2016benchmarking}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Learning a hierarchical model by meta-learning}{14}{subsection.2.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The agent setup of the modulated controller\nobreakspace  {}\cite  {frans2017meta},$\theta $ represents the master policy, which selects a sub-policy to be active. In the diagram, $\phi _3$ is the active sub-policy, and actions are taken according to its output.}}{14}{figure.2.2}}
\newlabel{review_mlsh_arch}{{2.2}{14}{The agent setup of the modulated controller~\cite {frans2017meta},$\theta $ represents the master policy, which selects a sub-policy to be active. In the diagram, $\phi _3$ is the active sub-policy, and actions are taken according to its output}{figure.2.2}{}}
\citation{riedmiller2018learning}
\citation{andrychowicz2017hindsight}
\citation{riedmiller2018learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Goal-directed learning method}{15}{subsection.2.2.4}}
\citation{mankowitz2018unicorn}
\citation{vezhnevets2017feudal}
\citation{shu2017hierarchical}
\citation{andreas2016modular}
\citation{vezhnevets2016strategic}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Other methods targeting at sparse environments}{16}{subsection.2.2.5}}
\@setckpt{chapter2}{
\setcounter{page}{17}
\setcounter{equation}{11}
\setcounter{enumi}{9}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{2}
\setcounter{subsection}{5}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{2}
\setcounter{table}{0}
\setcounter{NAT@ctr}{0}
\setcounter{Item}{9}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{28}
\setcounter{parentequation}{0}
\setcounter{float@type}{16}
\setcounter{lstnumber}{1}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{algorithm}{0}
\setcounter{ALG@line}{0}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{section@level}{2}
\setcounter{lstlisting}{0}
}
