%!TEX program = xelatex
%!TEX root = ./thesis.tex
\chapter{Discussion}
This thesis studies reinforcement learning solutions to robot control tasks with multi-modal state space and sparse reward functions.

Firstly, we have proposed a set of experiment environments. We have shown that contemporary deep reinforcement learning algorithms for continuous control cannot solve the proposed problems which have multi-modal state space. 

Secondly, we have proposed several techniques aiming to improve the performance of the contemporary flat reinforcement learning methods, including Wasserstein actor critic Kronecker-factored trust region policy optimization (W-KTR) method, exceptional advantage estimation, and robust concentric Gaussian mixture model. Some of them, especially the exceptional advantage estimation method, lead to improvement on the final performance of the agent. However, an efficient flat reinforcement learning solution to multi-modal problems in terms of the number of training steps is still missing. It is worth exploring in the future whether there is any technique that can solve the multi-modal problems without getting stuck at local minimums for such a long time.

Thirdly, we have proposed the flexible-scheduling hierarchical method for the tasks with multi-modal state space and sparse reward functions given a predefined set of source tasks. The experiments show positive results, indicating that the proposed method can solve the tasks in an end-to-end manner.

The experiments in this thesis only show preliminary results on a limited set of tasks. The capability of the method on general robot problems also needs to be verified in a bigger variety of problems in future works. The drawback of this hierarchical reinforcement learning framework is that it requires a properly pre-defined set of source tasks. Designing the source-task set is easy for the proposed problems, but we are not sure whether the job is feasible for real-world robot control tasks in general. Apart from that, we have not thoroughly investigated how the scheduling of decision policy training and switcher policy training could be designed. We have a pre-defined switcher policy at first, and schedule the training of decision policy and switcher policy into two separate phases in our experiments. Future works can investigate whether it is possible to train the decision policy and the switcher policy jointly. Apart from that, how to properly balance the switcher policy for better reinforcement learning performance and lower decision frequency of the decider policy is worth studying.

In conclusion, realistic continuous control tasks with multi-modal state space and sparse reward functions still pose a significant challenge to flat and hierarchical reinforcement learning methods. Our study proposes several novel solutions, but an efficient solution that achieves optimal performance without human intervention is still missing.