%!TEX program = xelatex
%!TEX root = ./thesis.tex
\chapter{Discussion}
This thesis studies reinforcement learning solutions to robot control tasks with multi-modal state space and sparse reward functions.

Firstly, we have proposed a set of experiment environments and shown that contemporary deep reinforcement learning algorithms for continuous control are not good at solving them. 

Secondly, we have proposed several techniques aiming to improve the performance of the contemporary flat reinforcement learning methods, including Wasserstein actor critic Kronecker-factored trust region policy optimization (W-KTR) method, exceptional advantage estimation, and robust concentric Gaussian mixture model. Some of them, especially the exceptional advantage estimation method, lead to improvement on the final performance of the agent. However, an efficient flat reinforcement learning solution is still missing. It is worth exploring in the future whether there could be any method that could completely solve the local minimum problem of the tasks with multi-modal state spaces.

Thirdly, we have proposed the flexible-scheduling hierarchical method for the tasks with multi-modal state space and sparse reward functions given a predefined set of source tasks. The experiments show positive results in that the proposed method can learn to solve the challenging tasks.

The experiments in this thesis only show preliminary results on a limited set of tasks. The capability of the method on general continuous control problems needs to be verified in future works. The drawback of this hierarchical reinforcement learning framework is that it requires a properly pre-defined set of source tasks. Designing the source-task set is easy for the proposed problems, but whether this is feasible for general realistic tasks remains a question. It is also worth studying whether it is possible to develop a more general formulation, which includes a infinite set of source tasks parameterized by a continuous space. Apart from that, we have not thoroughly investigated how the scheduling of the learning of the decision policy training and switcher policy training could be designed. We have a pre-defined fixed-period switcher policy at first, and schedule the training of decision policy and switcher policy into two separate phases. Future works can investigate whether it is possible to train the decision policy and the switcher policy jointly. Finally, how to properly balance the switcher policy for better reinforcement learning performance and better temporal abstraction is worth studying.

In conclusion, realistic continuous control tasks with multi-modal state space and sparse reward functions still pose a significant challenge. Our study proposes several novel methods, but an efficient reinforcement learning solution that achieves optimal performance is still missing.