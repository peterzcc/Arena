%!TEX program = xelatex
%!TEX root = ./thesis.tex
\chapter{Discussion}
This thesis studies reinforcement learning solutions to robot control tasks with multi-modal state space and sparse reward functions.

Firstly, we have proposed a set of experiment environments and shown that contemporary deep reinforcement learning algorithms for continuous control are not good at solving them. 

Secondly, we have proposed several techniques aiming to improve the performance of the contemporary flat reinforcement learning methods, including Wasserstein actor critic Kronecker-factored trust region policy optimization (W-KTR) method, exceptional advantage estimation, and robust concentric Gaussian mixture model. Some of them, especially the exceptional advantage estimation method, lead to improvement on the final performance of the agent. However, an efficient flat reinforcement learning solution to multi-modal state-space problems is still missing. It is worth exploring in the future whether there could be any method that could completely solve the local minimum problem of multi-modal state-space problems.

Thirdly, we have proposed the flexible-scheduling hierarchical method for the tasks with multi-modal state space and sparse reward functions given a predefined set of source tasks. The experiments show positive results in that the proposed method can learn to solve the tasks in an end-to-end manner.

The experiments in this thesis only show preliminary results on a limited set of tasks. The capability of the method on general robot problems also needs to be verified on a bigger variety of problems in future works. The drawback of this hierarchical reinforcement learning framework is that it requires a properly pre-defined set of source tasks. Designing the source-task set is easy for the proposed problems, but we are not sure whether the job is feasible for realistic tasks in general. Apart from that, we have not thoroughly investigated how the scheduling of the decision policy training and switcher policy training could be designed. We have a pre-defined switcher policy at first, and schedule the training of decision policy and switcher policy into two separate phases in our experiments. Future works can investigate whether it is possible to train the decision policy and the switcher policy jointly. Apart from that, how to properly balance the switcher policy for better reinforcement learning performance and lower decision frequency of the decider policy is worth studying.

In conclusion, realistic continuous control tasks with multi-modal state space and sparse reward functions still pose a significant challenge to flat and hierarchical reinforcement learning methods. Our study proposes several novel solutions, but an efficient solution that achieves optimal performance without human intervention is still missing.