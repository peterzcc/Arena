%!TEX program = xelatex
%!TEX root = ./thesis.tex
\chapter{Discussion}
This thesis studies reinforcement learning solutions to robot control tasks with multi-modal state space and sparse reward functions.

Firstly, we have proposed a set of experiment environments. We have shown that contemporary deep reinforcement learning algorithms for continuous control cannot solve the proposed problems with multi-modal state space. 

Secondly, we have proposed several techniques aiming to improve the performance of the contemporary flat reinforcement learning methods, including Wasserstein Actor Critic Kronecker-factored Trust Region Policy Optimization (W-KTR) method, exceptional advantage estimation, and robust concentric Gaussian mixture model. Some of them, especially exceptional advantage estimation, lead to improvement on the final performance of the agent. However, an efficient flat reinforcement learning solution to multi-modal problems in terms of number of training steps is still missing. It is worth exploring in the future if there is any machine learning technique that can solve the multi-modal problems without getting stuck at local minimum for such a long time.

Thirdly, we have proposed an autonomous hierarchical reinforcement learning method for the tasks with multi-modal state space and sparse reward functions given a predefined set of source tasks. The experiment shows positive result that the proposed method can solve the tasks in an end-to-end manner.

The experiment in this thesis only show preliminary results on a limited set of tasks. The capability of the method on general robot problems also needs to be verified in a bigger variety of problems in future works. The drawback of this hierarchical reinforcement learning framework is that it requires a properly pre-defined set of source tasks. Designing the source-task set is easy for the proposed problems, but we are not sure whether the job is feasible for real-world robot control tasks in general. Apart from that, we have not thoroughly investigated how the scheduling of decision policy training and switcher policy training could be designed. We design a dummy switcher policy at first, and separate the training of decision policy and switcher policy into two phases in our experiments. Future works can investigate whether it is possible to train the decision policy and the switcher policy jointly. Apart from that, how to properly encourage the switcher policy for producing longer execution time of actuator policies is worth studying.

In conclusion, realistic continuous control tasks with multi-modal state space and sparse reward functions still pose a significant challenge to flat and hierarchical reinforcement learning methods. Our study proposes several novel methods for solving a problem, but an efficient solution that achieves optimal performance without human intervention is still missing.