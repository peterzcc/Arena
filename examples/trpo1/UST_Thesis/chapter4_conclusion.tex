%!TEX program = xelatex
%!TEX root = ./thesis.tex
\section{Conclusion of Experimental Results}
We have proposed a set of reinforcement learning tasks that have multi-modal state space.s Some of them also have sparse reward functions. Control experiments have been done and the results show that the contemporary reinforcement learning algorithms are not good at solving these tasks.
We have proposed several techniques that aim to improve the performance of flat reinforcement learning methods on the proposed task. Firstly we propose the Wasserstein actor critic Kronecker-factored trust region policy optimization (W-KTR) method aiming to improve the learning performance. However, the experiment results on the W-KTR method show that the method does not significantly outperform other contemporary state-of-the-art methods. We have also proposed two techniques aiming to achieve better exploration performance. The first technique, namely exceptional advantage estimation, has proven to improve the final performance of flat reinforcement learning performance on the task movecont. The second technique, namely robust concentric Gaussian mixture model, also manages to solve the movecont task but is less efficient in terms of the total training time. 

%Although the above-mentioned methods are able to improve the performance of flat reinforcement learning methods on these tasks, a long training time and a large of training data are still required. Apart from that, tasks with both multi-modal state space and sparse reward functions could hardly be solved by flat reinforcement learning methods. The performance of the proposed method should better be verified in a bigger variety of tasks in future works.

We have also proposed the flexible-scheduling hierarchical method for the difficult sparse-reward problems. The domain randomization by cross-sampling initial states method has been proven to successfully learn the actuator policies of the source tasks. The final hierarchical agent achieves a good performance in the target tasks. Both the decision policy and the switcher policy are trained in an end-to-end manner. 