%!TEX program = xelatex
%!TEX root = ./thesis.tex
\chapter{Methodology}
\input{chapter3_envs.tex}
\input{chapter3_wass.tex}
\input{chapter3_exploration_regularization.tex}
\input{chapter3_exploration_mixture.tex}
\section{Hierarchical reinforcement learning architecture}
We propose to solve the reinforcement learning problem by a two-level hierarchical model. 

The hierarchical model consists of a top-level decider agent and a set of bottom-level actuator agents. The actuator agents' policies are trained im the source task environments. 

The decider agent takes an action at every time-step. It may either decide which actuator-policy should be executed, or simply skip and continue current actuator-policy. Therefore, assume there are $n_a$ sub-policies, the action space of the root agent is an $(n_a+1)$-discrete action space.

The observation space of the root agent consists of 2 parts, original statn (motion-sensor observation, image observation) and the meta state. The meta observation state the current sub-policy being executed and the number of time-steps since the last decision has been made.

Empirically, the decider agent is parameterized by two policy networks, $\theta_s$ and $\theta_d$. The network $\theta_s$, namely switcher network, outputs a binary action that decides whether the agent should simply continue using the current acting actuator policy, or switch to another policy based on the current state. The agent

The network $\theta_d$, namely decider network, outputs an $n_a$-discrete action space, that select the acting agent

The selected leaf agent executes the corresponding sub-policy and computes the primary actions the agent should take for the original environment.

 The overalll decision-making process of the decider agent is shown in Algorithm~\ref{hrl_decision_proc}.

\begin{algorithm}
\caption{The decider agent mechanism}\label{hrl_decision_proc}
\begin{algorithmic}%[1]
\Function{deciderAct}{self,$s_t$}
\State $a_{decider} \sim \pi_{decider}(s_t)$
 \If{$a_{decider} \neq 0$}
 \State $self.currentActuator \gets self.allA
ctuators[a_{decider}-1]$
 \EndIf
\State $a_{actuator} \gets self.currentActuator.act(s_t)$
\State \Return $a_{actuator}$
\EndFunction
\end{algorithmic}
\end{algorithm}


\section{Generalized advantage estimation for hierarchical reinforcement learning agents}
We propose a generalized advantage estimation method for decider agents hierarchical reinforcement learning agents. 

Assume that a decider agent makes decisions at time $t_1,t_2,\dots$, then the execution length of the decisions are $l_i = t_{i+1} - t_i, i=1,2,\dots$.

Then the definition of reward of the decider action at $t_i$ is given by:
\begin{align}
\bar{r}_{t_i} \defeq
 \sum_{l=0}^{t_{i+1}-t_i-1}
  \gamma^l r_{t_i+l}
\end{align}

Define the TD residual $\dv_{t_i}$ for $i=0,1, \dots$by:
\begin{align}
\dv_{t_i} & \defeq -V(s_{t_i}) + \bar{r}_{t_i} + \gamma^{t_{i+1}-t_i} V(s_{t_{i+1}})
\end{align}
Then the k-step advantage estimation is given by:
\begin{alignat}{2}
\hata_{t_i}^{(1)} & \defeq   \dv_{{t_i}} 
 &&=-V(s_{t_i}) + \bar{r}_{t_i} + \gamma^{t_{i+1}-t_i} V(s_{t_{i+1}})\\
\hata_{t_i}^{(2)} 
&\defeq \dv_{t_i} + \gamma^{t_{i+1}-t_i} \dv_{t_{i+1}} 
&&= -V(s_{t_i}) +\bar{r}_{t_i} + \gamma^{t_{i+1}-t_i} \bar{r}_{t_{i+1}} + \gamma^{t_{i+2}-t_i} V(t_{i+2}) 
\hata_t^{(3)} &\defeq \dv_{t} + \gamma \dv_{t+1} + \gamma^2 \dv_{t+2} &&= -V(s_t) + r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 V(s_{t+3}) \label{a3}
\end{alignat}
\begin{align}
\begin{split}
\hata_{t_i}^{(k)} 
&\defeq \sum_{d=0}^{k-1} 
\gamma^{t_{i+d}-t_i} \dv_{t_{i+d}} \\
&= -V(s_t) 
+\bar{r}_{t_i} + \gamma^{t_{i+1}-t_i} \bar{r}_{t_{i+1}} 
+ \dots 
+ \gamma^{t_{i+k-1}-t_i} \bar{r}_{t_{i+k-1}} 
+ \gamma^{t_{i+k}-t_i} V(s_{t_{i+k}})
\end{split}
\end{align}
We can define the unnormalized generalized advantage estimator as a exponentially-weighted sum of these k-step advantage estimators~\cite{schulman2015high}:
\begin{align}
\hata_{t_i}^{GAE_{unnorm}(\lambda)}
&\defeq  \hata_{t_i}^{(1)} + \lambda^{t_{i+1}-t_i}  \hata_{t_i}^{(2)} + \lambda^{t_{i+2}-t_i} \hata_{t_i}^{(3)} + \dots + \lambda^{t_{i+k-1}-t_i} \hata_{t_i}^{(k)} \nonumber \\
&=  \dv_{t_i} 
+ \lambda^{t_{i+1}-t_i} (\dv_{t_i} + \gamma^{t_{i+1}-t_i} \dv_{t_{i+1}} ) \\
&\ \ \ \ \  \ \ \ \ \ \ +\lambda^{t_{i+2}-t_i} (\dv_t + \gamma^{t_{i+1}-t_i} \dv_{t_{i+1}} + \gamma^{t_{i+2}-t_i} \dv_{t_{i+2}}) + \dots \nonumber \\
&\ \ \ \ \  \ \ \ \ \ \ +\lambda^{t_{i+k-1}-t_i}  \sum_{d=0}^{k-1} \gamma^{t_{i+d}-t_i} \dv_{t_{i+d}}\\
&= (
\dv_{t_i}  \sum_{b=0}^{k-1} \lambda^{t_{i+k-1}-t_i}
+\gamma^{t_{i+1}-t_i} \dv_{t_{i+1}} \sum_{b=1}^k \lambda^{t_{i+b}-t_i} \nonumber \\
&\ \ \ \ \  \ \ \ \ \ \ +\gamma^{t_{i+2}-t_i} \dv_{t_{i+2}} \sum_{b=2}^k \lambda^{t_{i+b}-t_i}
+\dots \\
&\ \ \ \ \  \ \ \ \ \ \ +\gamma^{t_{i+k-1}-t_i} \dv_{t_{i+k-1}} \lambda^{t_{i+k-1}-t_i})
\nonumber \\
&= \sum_{d=0}^{k-1} \dv_{t_i} \gamma^{t_{i+d}-t_i} \sum_{b=0}^{k-1} \lambda^{t_{i+b}-t_i}
\label{eq:gaelam1}
\end{align}
The normalized generalized advantage estimater is then given by:
\begin{align}
\hata_{t_i}^{GAE(\lambda)}
= \frac{\hata_{t_i}^{GAE_{unorm}(\lambda)}}{\sum_{b=0}^{k-1} \lambda^{t_{i+b}-t_i}}
\end{align}
However, the unormalized GAE estimator is usually used in practice instead of the normalized one, with a postprocessing step of batch normalization to adjust the scale of the advantages. This pratical method usually lead to large advantage scales for experience data at the beginning of the episodes and small scales for the experience data near episode ends.

