%!TEX program = xelatex
%!TEX root = ./thesis.tex

\section{Efficient training of actuator policies}
We have found that the straight-forward reuse of source task policies in the target task may not work.

For example, consider the target task as "dynamicg4" and the set of source tasks as "move0", "move1", "move2", "move3". When the goal direction in "dynamicg4" changes from $(-1,0)$ to $(1,0)$ the correct decider policy should be switching from "move1" to "move0". However, the actuator policy of "move0" fails to perform properly, because the initial state is sampled from the states generated by "move1" policy, rather than the original initial state distribution.

A straight-forward method to solve this problem is to perform joint training of actuator policy in the target task, with a uniform random decision policy and fixed-length switcher policy. However, this method makes the training much slower and much more ineffective.

There is a study \cite{tobin2017domain} that proposes a method to improve the robustness of policies. The method basically artificially adds variance to the environments, so that the trained policy will be able to deal with a larger variety of circumstances. However, this method doesn't work in our case. Because simply increasing the variance of initial states will unnecessarily lead the agent to start with many unstable states that would otherwise not be encountered, and it is infeasible to manually design the generative model for all the possible states in the target task.

\subsection{Domain randomization by cross-sampling initial states}
We propose a novel method for training robust actuator agents, namely domain randomization by cross-sampling initial states. We train all the source tasks simultaneously. During the training, each actuator agent keeps a experience buffer that store its state history. At the beginning of each episode, the actuator environment randomly selects the initial state from the set of its initial state distribution and the experience buffer of other tasks. This will guarantee that each actuator agents will encounter states generated by other agents. The proposed method is also more efficient than the direct joint training method, and doesn't require domain-specific engineering.