%!TEX program = xelatex
%!TEX root = ./thesis.tex
\section{Training of the Actuator Policies}
This section discusses the related techniques in training the actuator policies in the proposed hierarchical reinforcement learning formulation.
\subsection{Effective Training of Actuator Policies}
Directly applying the actuator policies trained in the source tasks to the target task is not likely to work, because the initial state distribution could be significantly different in the target task.
For example, consider the target task as dynamicg4 and the set of source tasks as move0, move1, move2, move3. When the target direction in dynamicg4 changes from $(-1,0)$ to $(1,0)$ the optimal decider policy should be switching from move1 to move0. However, the actuator policy of move0 could fail to perform well, because the initial state is sampled from the states generated by move1 policy. At such an initial state, the agent might be moving in the direction $(-1,0)$ in a high velocity and was not encountered during the training of move0.

A straight-forward method to solve this problem is to perform joint training of actuator policy in the target task with a uniform random decision policy and fixed-length switcher policy. However, this method increases the training time linearly with respect to the number of source tasks.

There is a study \cite{tobin2017domain} that proposes a technique to improve the robustness of policies. The method tries to artificially add variety to the environments, so that the trained policy will be able to deal with a larger variety of conditions. However, the method does not work in our case, because there is no simple way to increase the variety of the initial states in our problems without sampling degenerate states. 

\subsection{Domain Randomization by Cross-sampling Initial States}
We propose a novel method for training robust actuator agents, namely domain randomization by cross-sampling initial states. We train all the source tasks simultaneously, and each actuator agent keeps an experience buffer that stores its state history. At the beginning of each episode, the actuator environment randomly samples a state from the set of its initial state distribution and the experience buffer of other tasks. This will guarantee that each actuator agents will encounter states generated by other agents. As a result, the proposed method is more efficient than the direct joint training method.
\subsection{Synchronous Scheduling of Actuator Learning}
One of the problems related to the joint training of multiple tasks is `strategy collapse'~\cite{openai_2018}. This phenomenon refers to the status where the agents converge to sub-optimal policies due to their policy collectively forming a sub-optimal equilibrium. We propose a technique, namely synchronous scheduling of actuator learning, to solve this problem. The method actively controls the training schedule of actuator agents. The training of an actuator agent is paused if it largely surpasses the lowest performance, until all the other agents have reached the agent's performance level. This method requires a universal definition of performance level for all the source tasks, which is challenging when the source tasks are irrelevant and the episode rewards are of different scales. The method is suitable for our experiment settings, where the source tasks here are basically the same type of tasks with different specifications. 
\subsection{Prioritized Initial-state Memory Structure}
We propose another solution for preventing the problem of strategy collapse, namely prioritized initial-state memory structure. The method uses a randomized data structure to store the initial states. Instead of maintaining a fixed-size memory storage with the first-in-first-out criteria, the proposed memory structure replaces an existing data sample that is chosen at random when it receives a new data sample. As a result, the probability of a data sample that gets preserved is $(1-1/M)^\tau$, where $M$ is the memory size and $\tau$ is the elapsed time. Therefore, the proposed memory storage can cover samples from a much longer range of time compared to a fixed-size circular queue structure.

