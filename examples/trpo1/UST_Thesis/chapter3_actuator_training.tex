%!TEX program = xelatex
%!TEX root = ./thesis.tex
\section{Training of the actuator policies}
This section discuss the related techniques in training the actuator policies in the proposed hierarchical reinforcement learning formulation.
\subsection{Effective training of actuator policies}
Directly applying the actuator policies trained in the source tasks to the target task is not likely to work. Because the initial state distribution could have a significant difference.
For example, consider the target task as "dynamicg4" and the set of source tasks as "move0", "move1", "move2", "move3". When the goal direction in "dynamicg4" changes from $(-1,0)$ to $(1,0)$ the correct decider policy should be switching from "move1" to "move0". However, the actuator policy of "move0" could fail to perform well, because the initial state is sampled from the states generated by "move1" policy. At such initial state, the agent might be moving to the "direction 1" in a high velocity and was not encountered during the training of "move0".

A straight-forward method to solve this problem is to perform joint training of actuator policy in the target task with a uniform random decision policy and fixed-length switcher policy. However, this method increases the training time linearly with respective to the number of source tasks.

There is a study \cite{tobin2017domain} that proposes a technique to improve the robustness of policies. The method tries to artificially add variety to the environments, so that the trained policy will be able to deal with a larger variety of conditions. However, the method doesn't work in our case. Because there is no simple way to increase the variety of the initial states in our problems without sampling degenerate states. 

\subsection{Domain randomization by cross-sampling initial states}
We propose a novel method for training robust actuator agents, namely domain randomization by cross-sampling initial states. We train all the source tasks simultaneously. During the training, each actuator agent keeps an experience buffer that store its state history. At the beginning of each episode, the actuator environment randomly samples a state from the combination of its initial state distribution and the experience buffer of other tasks. This will guarantee that each actuator agents will encounter states generated by other agents. As a result, the proposed method is more efficient than the direct joint training method.

One of the problems related to the joint training of multiple tasks is 'strategy collapse'. This phenomenon refers to the status where the agents converge to sub-optimal policies due to their policy collectively form a sub-optimal equilibrium. We propose a technique, namely "synchronous scheduling of actuator learning ", to solve this problem, which actively controls the training schedule of actuator agents. The training of an actuator agent is paused if it largely surpasses the lowest performance, until all the other agents have reached the agent's performance level. This method requires a universal definition of performance level for all the source tasks, which is challenging when the source tasks are irrelevant and the episode rewards are in different scales. The method is suitable for our experiment settings, where the source tasks here are basically the same type of tasks with symmetrical specifications. Another solution could be increasing the time span of the experience buffer, so that an agent will be less influenced by the current status of other agents.

