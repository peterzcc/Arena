%!TEX program = xelatex
%!TEX root = ./thesis.tex
\section{Hierarchical Reinforcement Learning method}
\subsection{Hierarchical Reinforcement Learning Architecture}
We propose a two-level hierarchical model to solve the target problems. The hierarchical model consists of a top-level decider agent and a set of bottom-level actuator agents. The actuator agents' policies is a mapping from the state to the primary action space, and are trained in the source-task environments. The actuator policies are fixed during the training of the target task.

The decider agent takes an action at every time-step. It may either decide which actuator-policy should be executed, or continue using the current actuator policy that is decided earlier. Therefore, assume there are $n_a$ sub-policies, the action space of the decider agent is actually an $(n_a+1)$-discrete action space. The overalll decision-making process of the decider agent is shown in Algorithm~\ref{hrl_decision_proc}.

The observation space of the decider agent consists of 2 parts, the original multi-modal state and the meta state. The meta state contains information about the current sub-policy being executed and the number of time-steps since the last decision has been made.

The decider agent is parameterized by two policy networks, $\theta_s$ and $\theta_d$. The network $\theta_s$, namely switcher policy, outputs a scalar value that indicates whether the agent should simply continue using the current acting actuator policy, or terminate the current actuator policy and make the decision on the selection of actuator policy again. 
The network $\theta_d$, namely the decider policy, outputs an $n_a$-discrete action space. The output of the network represents the decision on the selection of the acting actuator policy. 



\begin{algorithm}
\caption{The decider agent mechanism}\label{hrl_decision_proc}
\begin{algorithmic}%[1]
\Function{deciderAct}{self,$s_t$}
\State $a_{decider} \sim \pi_{decider}(s_t)$
 \If{$a_{decider} \neq 0$}
 \State $self.currentActuator \gets self.allA
ctuators[a_{decider}-1]$
 \EndIf
\State $a_{actuator} \gets self.currentActuator.act(s_t)$
\State \Return $a_{actuator}$
\EndFunction
\end{algorithmic}
\end{algorithm}


\subsection{Generalized advantage estimation for the decider agent}
We propose a generalized advantage estimation method for the decider agent of the proposed hierarchical reinforcement learning model. 

Assume that a decider agent makes decisions at time $t_1,t_2,\dots$, then the execution length of the corresponding actuator policies are $l_i = t_{i+1} - t_i, i=1,2,\dots$.

The definition of reward of the decider action at $t_i$ is given by:
\begin{align}
\bar{r}_{t_i} \defeq
 \sum_{l=0}^{t_{i+1}-t_i-1}
  \gamma^l r_{t_i+l}
\end{align}

Define the TD residual $\dv_{t_i}$ for $i=0,1, \dots$by:
\begin{align}
\dv_{t_i} & \defeq   \bar{r}_{t_i} + \gamma^{t_{i+1}-t_i} V(s_{t_{i+1}}) -V(s_{t_i})
\end{align}
Then the k-step advantage estimation is given by:
\begin{alignat}{2}
\hata_{t_i}^{(1)} 
& \defeq   \dv_{{t_i}} 
 &&= \bar{r}_{t_i} + \gamma^{t_{i+1}-t_i} V(s_{t_{i+1}})-V(s_{t_i}) \\
\hata_{t_i}^{(2)} 
&\defeq \dv_{t_i} + \gamma^{t_{i+1}-t_i} \dv_{t_{i+1}} 
&&= \bar{r}_{t_i} + \gamma^{t_{i+1}-t_i} \bar{r}_{t_{i+1}} + \gamma^{t_{i+2}-t_i} V(t_{i+2})-V(s_{t_i})  \\
\hata_t^{(3)} 
&\defeq \dv_{t} + \gamma \dv_{t+1} + \gamma^2 \dv_{t+2} 
&&=  r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 V(s_{t+3}) -V(s_t)  \label{a3}
\end{alignat}
\begin{align}
\begin{split}
\hata_{t_i}^{(k)} 
&\defeq \sum_{d=0}^{k-1} 
\gamma^{t_{i+d}-t_i} \dv_{t_{i+d}} \\
&= \bar{r}_{t_i} + \gamma^{t_{i+1}-t_i} \bar{r}_{t_{i+1}} 
+ \dots 
+ \gamma^{t_{i+k-1}-t_i} \bar{r}_{t_{i+k-1}} 
+ \gamma^{t_{i+k}-t_i} V(s_{t_{i+k}})-V(s_t) 
\end{split}
\end{align}
We can define the unnormalized generalized advantage estimator as a exponentially-weighted sum of these k-step advantage estimators~\cite{schulman2015high}:
\begin{align}
\hata_{t_i}^{GAE_{unnorm}(\lambda)}
&\defeq  \hata_{t_i}^{(1)} + \lambda^{t_{i+1}-t_i}  \hata_{t_i}^{(2)} + \lambda^{t_{i+2}-t_i} \hata_{t_i}^{(3)} + \dots + \lambda^{t_{i+k-1}-t_i} \hata_{t_i}^{(k)} \nonumber \\
&=  \dv_{t_i} 
+ \lambda^{t_{i+1}-t_i} (\dv_{t_i} + \gamma^{t_{i+1}-t_i} \dv_{t_{i+1}} ) \\
&\ \ \ \ \  \ \ \ \ \ \ +\lambda^{t_{i+2}-t_i} (\dv_t + \gamma^{t_{i+1}-t_i} \dv_{t_{i+1}} + \gamma^{t_{i+2}-t_i} \dv_{t_{i+2}}) + \dots \nonumber \\
&\ \ \ \ \  \ \ \ \ \ \ +\lambda^{t_{i+k-1}-t_i}  \sum_{d=0}^{k-1} \gamma^{t_{i+d}-t_i} \dv_{t_{i+d}}\\
&= (
\dv_{t_i}  \sum_{b=0}^{k-1} \lambda^{t_{i+k-1}-t_i}
+\gamma^{t_{i+1}-t_i} \dv_{t_{i+1}} \sum_{b=1}^k \lambda^{t_{i+b}-t_i} \nonumber \\
&\ \ \ \ \  \ \ \ \ \ \ +\gamma^{t_{i+2}-t_i} \dv_{t_{i+2}} \sum_{b=2}^k \lambda^{t_{i+b}-t_i}
+\dots \\
&\ \ \ \ \  \ \ \ \ \ \ +\gamma^{t_{i+k-1}-t_i} \dv_{t_{i+k-1}} \lambda^{t_{i+k-1}-t_i})
\nonumber \\
&= \sum_{d=0}^{k-1} \dv_{t_i} \gamma^{t_{i+d}-t_i} \sum_{b=0}^{k-1} \lambda^{t_{i+b}-t_i}
\label{eq:gaelam1}
\end{align}
The normalized generalized advantage estimater is then given by:
\begin{align}
\hata_{t_i}^{GAE(\lambda)}
= \frac{\hata_{t_i}^{GAE_{unorm}(\lambda)}}{\sum_{b=0}^{k-1} \lambda^{t_{i+b}-t_i}}
\end{align}
However, the unormalized GAE estimator is usually used in practice instead of the normalized one, with a postprocessing step of batch normalization to adjust the scale and bias of the advantages. 

\subsection{Training of the Switcher Policy}
We train the switcher agent using a flat reinforcement learning method with respective to the return of the target task. However, an extra loss term is added so that the switcher network will be encouraged to produce termination signals less frequently.
\begin{align}
J'(\theta_s) = J'(\theta_s) + \mathbb{E}[\pi_s(s)]
\end{align}
Where $\pi_s$ is the switcher policy distribution. The second term is the basically expected frequency that the switcher network produces the positive output.

