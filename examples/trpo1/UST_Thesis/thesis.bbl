% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\newcommand{\noopsort}[1]{} \newcommand{\printfirst}[2]{#1}
  \newcommand{\singleletter}[1]{#1} \newcommand{\switchargs}[2]{#2#1}
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{heess2016learning}
N.~Heess, G.~Wayne, Y.~Tassa, T.~Lillicrap, M.~Riedmiller, and D.~Silver,
  ``Learning and transfer of modulated locomotor controllers,'' \emph{arXiv
  preprint arXiv:1610.05182}, 2016, \url{https://arxiv.org/pdf/1610.05182}.

\bibitem{frans2017meta}
K.~Frans, J.~Ho, X.~Chen, P.~Abbeel, and J.~Schulman, ``Meta learning shared
  hierarchies,'' \emph{arXiv preprint arXiv:1710.09767}, 2017.

\bibitem{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski \emph{et~al.},
  ``Human-level control through deep reinforcement learning,'' \emph{Nature},
  vol. 518, no. 7540, pp. 529--533, 2015.

\bibitem{silver2016mastering}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~Van Den~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot \emph{et~al.},
  ``Mastering the game of go with deep neural networks and tree search,''
  \emph{Nature}, vol. 529, no. 7587, pp. 484--489, 2016.

\bibitem{heinrich2016deep}
J.~Heinrich and D.~Silver, ``Deep reinforcement learning from self-play in
  imperfect-information games,'' \emph{arXiv preprint arXiv:1603.01121}, 2016.

\bibitem{duan2016benchmarking}
Y.~Duan, X.~Chen, R.~Houthooft, J.~Schulman, and P.~Abbeel, ``Benchmarking deep
  reinforcement learning for continuous control,'' in \emph{International
  Conference on Machine Learning}, 2016, pp. 1329--1338.

\bibitem{sutton1998reinforcement}
R.~S. Sutton and A.~G. Barto, \emph{Reinforcement learning: An
  introduction}.\hskip 1em plus 0.5em minus 0.4em\relax MIT press Cambridge,
  1998, vol.~1, no.~1.

\bibitem{barto2003recent}
A.~G. Barto and S.~Mahadevan, ``Recent advances in hierarchical reinforcement
  learning,'' \emph{Discrete Event Dynamic Systems}, vol.~13, no.~4, pp.
  341--379, 2003.

\bibitem{taylor2009transfer}
M.~E. Taylor and P.~Stone, ``Transfer learning for reinforcement learning
  domains: A survey,'' \emph{Journal of Machine Learning Research}, vol.~10,
  no. Jul, pp. 1633--1685, 2009.

\bibitem{wu2017scalable}
Y.~Wu, E.~Mansimov, R.~B. Grosse, S.~Liao, and J.~Ba, ``Scalable trust-region
  method for deep reinforcement learning using kronecker-factored
  approximation,'' in \emph{Advances in Neural Information Processing Systems},
  2017, pp. 5285--5294.

\bibitem{schulman2015high}
J.~Schulman, P.~Moritz, S.~Levine, M.~Jordan, and P.~Abbeel, ``High-dimensional
  continuous control using generalized advantage estimation,'' \emph{arXiv
  preprint arXiv:1506.02438}, 2015.

\bibitem{schulman2015trust}
J.~Schulman, S.~Levine, P.~Moritz, M.~I. Jordan, and P.~Abbeel, ``Trust region
  policy optimization,'' \emph{CoRR, abs/1502.05477}, 2015,
  \url{http://www.jmlr.org/proceedings/papers/v37/schulman15.pdf}.

\bibitem{openaigym}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba, ``Openai gym,'' 2016.

\bibitem{schulman2017proximal}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov, ``Proximal
  policy optimization algorithms,'' \emph{arXiv preprint arXiv:1707.06347},
  2017.

\bibitem{lillicrap2015continuous}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and D.~Wierstra, ``Continuous control with deep reinforcement
  learning,'' \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem{wang2016sample}
Z.~Wang, V.~Bapst, N.~Heess, V.~Mnih, R.~Munos, K.~Kavukcuoglu, and
  N.~de~Freitas, ``Sample efficient actor-critic with experience replay,''
  \emph{arXiv preprint arXiv:1611.01224}, 2016.

\bibitem{nachum2017trust}
O.~Nachum, M.~Norouzi, K.~Xu, and D.~Schuurmans, ``Trust-pcl: An off-policy
  trust region method for continuous control,'' \emph{arXiv preprint
  arXiv:1707.01891}, 2017.

\bibitem{sutton1999between}
R.~S. Sutton, D.~Precup, and S.~Singh, ``Between mdps and semi-mdps: A
  framework for temporal abstraction in reinforcement learning,''
  \emph{Artificial intelligence}, vol. 112, no.~1, pp. 181--211, 1999.

\bibitem{parr1998reinforcement}
R.~Parr and S.~J. Russell, ``Reinforcement learning with hierarchies of
  machines,'' in \emph{Advances in neural information processing systems},
  1998, pp. 1043--1049.

\bibitem{dietterich2000hierarchical}
T.~G. Dietterich, ``Hierarchical reinforcement learning with the maxq value
  function decomposition,'' \emph{J. Artif. Intell. Res.(JAIR)}, vol.~13, pp.
  227--303, 2000.

\bibitem{mcgovern2001automatic}
A.~McGovern and A.~G. Barto, ``Automatic discovery of subgoals in reinforcement
  learning using diverse density,'' 2001.

\bibitem{hengst2002discovering}
B.~Hengst, ``Discovering hierarchy in reinforcement learning with hexq,'' in
  \emph{ICML}, vol.~2, 2002, pp. 243--250.

\bibitem{riedmiller2018learning}
M.~Riedmiller, R.~Hafner, T.~Lampe, M.~Neunert, J.~Degrave, T.~Van~de Wiele,
  V.~Mnih, N.~Heess, and J.~T. Springenberg, ``Learning by playing-solving
  sparse reward tasks from scratch,'' \emph{arXiv preprint arXiv:1802.10567},
  2018, \url{https://arxiv.org/pdf/1802.10567}.

\bibitem{andrychowicz2017hindsight}
M.~Andrychowicz, D.~Crow, A.~Ray, J.~Schneider, R.~Fong, P.~Welinder,
  B.~McGrew, J.~Tobin, O.~P. Abbeel, and W.~Zaremba, ``Hindsight experience
  replay,'' in \emph{Advances in Neural Information Processing Systems}, 2017,
  pp. 5055--5065,
  \url{http://papers.nips.cc/paper/7090-hindsight-experience-replay.pdf}.

\bibitem{mankowitz2018unicorn}
D.~J. Mankowitz, A.~{\v{Z}}{\'\i}dek, A.~Barreto, D.~Horgan, M.~Hessel,
  J.~Quan, J.~Oh, H.~van Hasselt, D.~Silver, and T.~Schaul, ``Unicorn:
  Continual learning with a universal, off-policy agent,'' \emph{arXiv preprint
  arXiv:1802.08294}, 2018.

\bibitem{vezhnevets2017feudal}
A.~S. Vezhnevets, S.~Osindero, T.~Schaul, N.~Heess, M.~Jaderberg, D.~Silver,
  and K.~Kavukcuoglu, ``Feudal networks for hierarchical reinforcement
  learning,'' \emph{arXiv preprint arXiv:1703.01161}, 2017.

\bibitem{shu2017hierarchical}
T.~Shu, C.~Xiong, and R.~Socher, ``Hierarchical and interpretable skill
  acquisition in multi-task reinforcement learning,'' \emph{arXiv preprint
  arXiv:1712.07294}, 2017.

\bibitem{andreas2016modular}
J.~Andreas, D.~Klein, and S.~Levine, ``Modular multitask reinforcement learning
  with policy sketches,'' \emph{arXiv preprint arXiv:1611.01796}, 2016.

\bibitem{vezhnevets2016strategic}
A.~Vezhnevets, V.~Mnih, S.~Osindero, A.~Graves, O.~Vinyals, J.~Agapiou
  \emph{et~al.}, ``Strategic attentive writer for learning macro-actions,'' in
  \emph{Advances in neural information processing systems}, 2016, pp.
  3486--3494.

\bibitem{henderson2017matters}
P.~Henderson, R.~Islam, P.~Bachman, J.~Pineau, D.~Precup, and D.~Meger, ``Deep
  reinforcement learning that matters,'' \emph{arXiv preprint
  arXiv:1709.06560}, 2017.

\bibitem{villani2003topics}
C.~Villani, \emph{Topics in optimal transportation}.\hskip 1em plus 0.5em minus
  0.4em\relax American Mathematical Soc., 2003, no.~58.

\bibitem{chafai}
\BIBentryALTinterwordspacing
D.~CHAFA√è, ``Wasserstein distance between two gaussians.'' [Online].
  Available:
  \url{http://djalil.chafai.net/blog/2010/04/30/wasserstein-distance-between-two-gaussians/}
\BIBentrySTDinterwordspacing

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,''
  \emph{arXiv preprint arXiv:1412.6980}, 2014.

\end{thebibliography}
