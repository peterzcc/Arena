%!TEX program = xelatex
%!TEX root = ./thesis.tex

\section{Hierarchical reinforcement learning methods for multi-modality and sparse environments}
This section will discuss the experiment results on the proposed hierarchical reinforcement learning model.

There are two target tasks to be solved in our experiment setting: "dynamicg8" and "reachcont". The target task "dynamicg8" is a representative problem that has multi-modality state-space, and the target task "reachcont" is a typical problem that has both multi-modality state-space and sparse reward signal. The set of source tasks is  $\{move0, move1, \dots, move7 \}$ for both target tasks.

The problem of hierarchical reinforcement learning consists of two parts: the training of actuator agents and the decider agent. The two problems will be discussed in the following sections.


\subsection{Training the actuator agents with domain randomization by cross-sampling initial states}
As is discussed previously, the learning of the actuator agent of a source task from the source task set is not always the same problem as training a flat reinforcement learning agent for that single task. The initial states generated by actuator agents for other source tasks must be handled, but is usually not encountered in the original source task.

We proposed that the method "domain randomization by cross-sampling initial states" can handle the problem of novel initial states. The performance of this method is experimented in this section. We train the set of source tasks $\{move0, move1, \dots, move7 \}$ simultaneously in this experiment.  

The result is plotted in Figure~\ref{rec_8task_training}, which shows that the actuator agents for some tasks get stuck at sub-optimal scores and fail to solve the corresponding source tasks. As is discussed previously, this problem could be due to the interference between actuator agents.

\begin{figure}[!htbp]
	\includegraphics[width=\textwidth]{images/rec_8task_training.pdf}
	\centering
	\caption{Performance of actuator agents with domain randomization by cross-sampling initial states, the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 200 episodes}\label{rec_8task_training}
\end{figure}

Therefore, we proposed the "synchronous scheduling of actuator learning" method to prevent the learning progress of any actuator agents from lagging behind too far from others. The experiment results of the proposed "synchronous scheduling of actuator learning" is shown in Figure \ref{rec_sync_training}. In this experiments, the policy training of agents who outperforms the global lowest-performance by 1000 is paused until all other actuator agents outperform this agent. The result shows that all the actuator agents reach a final performance of around 6000 although their performance diverge initially. This has verified that the proposed method can successfully prevent the trapped sub-optimal actuator agents. 

\begin{figure}[!htbp]
	\includegraphics[width=\textwidth]{images/rec_sync_training.pdf}
	\centering
	\caption{Performance of actuator agents using "synchronous scheduling of actuator learning" , the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 200 episodes}\label{rec_sync_training}
\end{figure}

We consider that the learning of source tasks is successfully solved by the proposed domain randomization by cross-sampling initial states with  synchronous scheduling of actuator learning method.
\subsection{Training the decider agent}
The training of the decider agent consists of two parts: the training of the decision policy and the the switcher policy. We train the two parts in separate phases. At first, the switcher policy is initialized so that it outputs a termination signal whenever the current decision policy has been executed for no less than $l_c$ time steps, which is set to 10 in our experiments. The decision policy is trained with the switcher policy being fixed. After the the performance of the decision policy becomes stable, the switcher policy is then trained with the decision policy fixed.
\subsubsection{Decision policy training}
The performance of the experiment on the training of decision policy on "dynamicg8" is shown in Figure~\ref{fig:rec_dynamicg8_decider_subt10}. The result shows that the agent is able to achieve a reasonable performance of around 2500, while it is not considered optimal.

The performance of the experiment on the training of decision policy on task "reachcont" is shown in Figure~\ref{fig:rec_reachc05_decider_subt10}. The agent achieves a final score of around 0.75 and is considered.


\begin{figure}[!htbp]
\centering
\includegraphics[width=\linewidth]{rec_dynamicg8_decider_subt10}
\caption{Decision policy training performance of the task "dynamicg8", the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 32 episodes}
\label{fig:rec_dynamicg8_decider_subt10}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=1\linewidth]{rec_reachc05_decider_subt10.pdf}
\caption{Decision policy training performance of the task "reachcont", the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 200 episodes}
\label{fig:rec_reachc05_decider_subt10}
\end{figure}

The result of the training of the decision policy shows that the hierarchical reinforcement learning agent is able to successfully learn the decision policy. The result is as expected because previous works in SMDP have studied intensively on this problem.



