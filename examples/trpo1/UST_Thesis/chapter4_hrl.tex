%!TEX program = xelatex
%!TEX root = ./thesis.tex

\section{Hierarchical reinforcement learning methods for multi-modality and sparse environments}
This section discusses the experiment results on the proposed hierarchical reinforcement learning model.

The experiment is done on a set of target tasks: "dynamicg8" and "reachcont". The target task "dynamicg8" is a typical problem that has multi-modality state-space and smooth reward signal, and the target task "reachcont" is a typical problem that has both multi-modality state-space and sparse reward signal. The set of source tasks is  $\{move0, move1, \dots, move7 \}$ for the target tasks.

The problem of hierarchical reinforcement learning consists of two parts: the training of actuator agents and the decider agent. The two problems will be discussed in separate sections.


\subsection{Training the actuator agents with domain randomization by cross-sampling initial states}
As is discussed previously, the learning of the actuator agent of a source task from the source task set is not always the same problem as training a flat reinforcement learning agent for that single task. The initial states generated by actuator agents for other source tasks must be handled, but is usually not encountered in the original source task.

We proposed that the method "domain randomization by cross-sampling initial states" can handle the problem of novel initial states. The performance of this method is tested in this section. We use the same configuration as in section \ref{sec_exp_move0} for each actuator policy.  

The experiment result on the performance of all the actuator agents is plotted in Figure~\ref{rec_8task_training}. It can be seen that of the actuator agents get stuck at sub-optimal performance levels. 

\begin{figure}[!htbp]
	\includegraphics[width=\textwidth]{images/rec_8task_training.pdf}
	\centering
	\caption{Performance of actuator agents with domain randomization by cross-sampling initial states, the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 200 episodes}\label{rec_8task_training}
\end{figure}

Therefore, the proposed "synchronous scheduling of actuator learning" method aims to prevent this problem. The experiment results of the technique is shown in Figure \ref{rec_sync_training}. In this experiment, the policy training of the actuator agents who outperforms the global lowest-performance by 1000 is paused until all other actuator agents outperform this agent. The result shows that all the actuator agents are able to reach a final performance of around 6000 although their performance diverge initially. This has verified that the proposed method can successfully prevent any actuator agents getting stuck at sub-optimal performance levels. 

\begin{figure}[!htbp]
	\includegraphics[width=\textwidth]{images/rec_sync_training.pdf}
	\centering
	\caption{Performance of actuator agents using "synchronous scheduling of actuator learning" , the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 200 episodes}\label{rec_sync_training}
\end{figure}

Therefore the learning of source tasks is successfully solved by the proposed technique of domain randomization by cross-sampling initial states with synchronous scheduling of actuator learning method.
\subsection{Training the decider agent}
The training of the decider agent consists of two phases: the training of the decision policy and the the switcher policy. We train them in two separate phases. In the first phase, the switcher policy is initialized so that it outputs a termination signal whenever the current decision policy has been executed for more than $l_c$ time steps, which is set to 10 in our experiments. The decision policy is trained with the switcher policy being fixed. After the the performance of the decision policy converges, the switcher policy is then trained with the decision policy fixed.

\subsubsection{Phase 1: Decision policy training}
The configuration of neural network parameters is the same as the ACKTR agent in section \ref{sec_multi_modal_flat} except that the type of distribution is categorical distribution instead of Gaussian distribution.
The experiment result on the performance of of the decision policy training on "dynamicg8" is shown in Figure~\ref{fig:rec_dynamicg8_decider_subt10}. The result shows that the agent is able to achieve a performance of around 2900. The performance is far from the theoritically optimal score of 6000, but the result shows that the agent could at least learn a meaningful policy.

The performance of the experiment on the training of decision policy on task "reachcont" is shown in Figure~\ref{fig:rec_reachc05_decider_subt10}. The agent achieves a average reward of around 0.75, and is considered having solved the task.


\begin{figure}[!htbp]
\centering
\includegraphics[width=\linewidth]{rec_dynamicg8_decider_subt10}
\caption{Decision policy training performance of the task "dynamicg8", the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 32 episodes}
\label{fig:rec_dynamicg8_decider_subt10}
\end{figure}


\begin{figure}
\centering
\includegraphics[width=1\linewidth]{rec_reachc05_decider_subt10.pdf}
\caption{Decision policy training performance of the task "reachcont", the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 200 episodes}
\label{fig:rec_reachc05_decider_subt10}
\end{figure}

The result of the training of the decision policy shows that the hierarchical reinforcement learning agent is able to achieve a reasonable level of performance. The result is as expected because previous works in SMDP have been studied intensively.

\subsubsection{Phase 2: Switcher policy training}
At the beginning of this phase, the switcher policy is re-initialized randomly so that it outputs a termination signal with a probability around 0.5. This reinitialized switcher policy is different from the switcher policy in phase 1 which outputs a termination signal deterministically after the current actuator policy has been executed for a pre-defined number of steps. The switcher policy is then trained from scratch.

The experiment result on the performance during the training of switcher policy on the task "dynamicg8" is shown in Figure~\ref{fig:rec_dynamicg8_switcher}. The decider agent appears to achieve a relatively stable performance during the training of the switcher policy, and even slightly improves the performance level.

The average execution length of the decision policies is plotted in Figure~\ref{fig:rec_dynamicg8_avesubt}. The agent manages to find a better switcher policy since the final average execution length becomes around $10$ while it was around $5$ in phase 1. It means that the time scale of the decision policy is reduced.

Therefore, the training of switcher policy beneficial in the sense that the agent can maintain a good performance. The frequency of invoking decision policy is also reduced.
\begin{figure}[!htbp]
	\centering
	\includegraphics[width=\linewidth]{rec_dynamicg8_switcher}
	\caption{Switcher policy training performance of the task "dynamicg8", the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 32 episodes}
	\label{fig:rec_dynamicg8_switcher}
\end{figure}

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=\linewidth]{rec_dynamicg8_avesubt}
	\caption{Average execution length of the actions of decision policies of the task "dynamicg8", the x-axis is the number of million time-steps and the y-axis is the decision policy's average execution length of the last batch.}
	\label{fig:rec_dynamicg8_avesubt}
\end{figure}

The experiment result on the performance during of switcher policy on the task "dynamicg8" is shown in Figure~\ref{fig:rec_reachcont_switcher}. The final performance has been improved from $0.75$ in the decision policy training to around $0.81$. The training of switcher policy appears to beneficial to the performance of the target task.

The average execution length of the decision policies is plotted in Figure~\ref{fig:rec_dynamicg8_avesubt}. The final average value of the execution length appears to be reduced to $1.6$, compared to $5$ in the decision policy training phase.

Therefore, the switcher policy is able to improve the reinforcement learning performance at the expense of increasing the decision frequency of the decision policy.

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=\linewidth]{rec_reachcont_switcher}
	\caption{Switcher policy training performance of the task "reachcont", the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 32 episodes}
	\label{fig:rec_reachcont_switcher}
\end{figure}

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=\linewidth]{rec_reachcont_switcher_subt}
	\caption{Average execution length of the actions of decision policies of the task "reachcont", the x-axis is the number of million time-steps and the y-axis is the decision policy's average execution length of the last batch.}
	\label{fig:rec_reachcont_switcher_subt}
\end{figure}

The above experiment results a policy that controls the scheduling intervals of the decision policy can actually be trained in an end-to-end manner, and the proposed hierarchal reinforcement learning method is able to solve the proposed multi-modal sparse tasks.