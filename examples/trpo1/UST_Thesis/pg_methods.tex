%!TEX program = xelatex
%!TEX root = ./thesis.tex
\section{Policy Gradient Methods}
Policy gradient methods are a class of reinforcement learning methods that learn a parametrized policy that select actions without consulting any value estimates, compared to the class of value-based reinforcement learning methods ~\cite{sutton1998reinforcement}. A value function may be still used to learn the policy parameter, but not for selecting actions. The vast majority of deep reinforcement learning methods for continuous control belong to the class of policy gradient methods instead of methods that are based on action-value functions, because it is infeasible to compute the optimal action from a general action-value function with a continuous action space.
Policy gradient methods optimize a parametrized policy model $\pi_\theta (a|s), a\in A, s\in S$, so that the expected return:
$$
    J(\theta) = \mathbb{E}\big[\sum_{t=0}^T\gamma^t r_t \big]
$$
is maximized. The constant $\gamma$ is a discount factor and $T$ is the episode length.

Policy gradient methods maximize the expected return iteratively by estimating the gradient $g := \nabla  \mathbb{E}\big[\sum_{t=0}^T\gamma^t r_t \big]$ , which has the form \cite{schulman2015high}:
\begin{equation} g = \mathbb{E}_{t,s_t,a_t} \big[
\Psi_t \frac{\nabla_\theta \pi_\theta(a_t|s_t) }{q(a_t|s_t)}
\big]
\label{pg_def}\end{equation}
where $q(.)$ is the distribution that generates the samples. If the samples are generated by the policy $\pi_\theta$, then policy gradient can be simplified using the REINFORCE trick \cite{williams1992simple}:
\begin{equation} g = \mathbb{E}_{t,s_t,a_t} \big[
\Psi_t \frac{\nabla_\theta \pi_\theta(a_t|s_t) }{\pi_\theta(a_t|s_t)}
\big]
=\mathbb{E}_{t,s_t,a_t} \big[
\Psi_t \nabla_\theta \log \pi_\theta(a_t|s_t) 
\big]
\label{pg_def_onpolicy}\end{equation}


The term $\Psi_t$ may be one of the following~\cite{schulman2015high}:
\begin{enumerate}
    \item $\sum_{t=0}^{\infty} r_t$  : expected total return
    \item $\sum_{t'=t}^{\infty} r_t$  : expected return following $a_t$
    \item $\sum_{t'=t}^{\infty} [r_t - b(s_t)]$
    \item $Q_\pi (s_t, a_t)$
    \item $A_\pi (s_t, a_t)$
    \item $ \hat{A}_t^{(1)}:=  \delta_t = r_t + V_\pi (s_{t+1}) - V_\pi (s_{t})$ : 1-step TD residual
    \item $ \hat{A}_t^{(2)} :=  \delta_t + \gamma \delta_{t+1}= r_t +\gamma r_{t+1} + \gamma^2 V_\pi (s_{t+2}) - V_\pi (s_{t})$ : 2-step TD residual
    \item $\hat{A}_t^{(k)} := \sum_{l=0}^{k-1} \gamma^{l} \delta_{t+l} =   r_t + \gamma r_{t+1} + \dots + \gamma^{k-1} r_{t+k-1} + \gamma^{k} V(s_{t+k}) -V(s_t)$ : k-step TD residual
    \item $GAE(\lambda) = \frac{1}{(1+\lambda+\lambda^2+\dots)} \big(\hat{A}_t^{(1)} + \lambda \hat{A}_t^{(2)}+ \lambda^2 \hat{A}_t^{(3)} +\dots \big) 
    \approx \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}$ , where $\lambda \in [0,1]$: the generalized advantage estimator proposed by \cite{schulman2015high}
\end{enumerate}
Many policy gradient methods have been proposed to solve reinforcement learning problems for continuous control. They can be classified into two categories: on-policy methods and off-policy methods. On-policy methods perform gradient updates only on the data sampled by the current policy, while off-policy methods could use the data that is generated by any other distributions. Off-policy methods are generally more sample efficient, but they need to handle the problems due to the difference of sampling distribution and the current policy. Apart from that, many reinforcement learning techniques that are valid for on-policy methods can not be used in off-policy methods. Therefore, we will mainly focus on on-policy policy gradient methods. The following sections will discuss some state-of-art policy gradient methods in details.
\subsection{Trust Region Policy Optimization}
The method proposed by \cite{schulman2015trust}, namely Trust Region Policy Optimization (TRPO) is one of the early works in deep reinforcement learning for continuous control. The method formulates the policy optimization problem as a constraint optimization problem:
\begin{equation}
    \begin{aligned}
&    \underset{\theta}{\text{maximize}} 
&& J(\theta) \\
& \text{subject to } 
&& \overline{D_{KL}}(\pi_{\theta_{old}}\|\pi_\theta) \leq \delta_{KL}\label{trpo_obj}\end{aligned}
\end{equation}
where $\pi_{\theta_{old}}$ is the old policy parameter before performing the parameter update, and $\overline{D_{KL}}$ is the average KL divergence over the samples, $\delta_{KL}$ is the hyper-parameter that control the step size of updates.

The method solves this problem through 4 steps. The first step computes $g$ following the Equation \ref{pg_def}. 
The second step computes the Riemannian metric tensor of the parameter space of the policy model $A = H_{\theta}\left(\overline{D_{KL}}(\pi_{\theta_{old}}\|\pi_\theta)\right)$ , where $H_{\theta}(.)$ denotes the Hessian matrix with respect to $\theta$. 
The third step obtains $s$ which is the natural gradient update direction $s=A^{-1}g$ by solving $g=As$ through the conjugate gradient algorithm.
The fourth step computes the maximum step-size $\beta= \sqrt{2\delta_{KL}/s^TAs}$ and performs a line search to ensure the improvement of the objective function.

The method manages to solve some simple robot control tasks in \cite{openaigym} within a few millions of training samples. However, both the computation of the Hessian matrix and the conjugate gradient algorithm are infeasible for a neural network model with nontrivial size.

\subsection{Kronecker-factored Trust Region Policy Gradient}
The work of \cite{wu2017scalable} proposes to reduce the computation time solving the natural gradient by the Kronecker-factored approximation method. The proposed algorithm is named Actor-critic  Kronecker-factored Trust Region method (ACKTR).
The Fisher Information Matrix $F=\mathbb{E}[\nabla_\theta L \nabla_\theta L^T]$, where $L=\log\pi(a|s)$ is used here to approximate the Riemannian metric tensor.
The matrix is approximated by:
\begin{equation*}
    F=\mathbb{E}[\nabla_\theta L \nabla_\theta L^T] = \mathbb{E}[aa^T \otimes \nabla_s L \nabla_s L^T ] 
    \approx \mathbb{E}[aa^T] \otimes \mathbb{E}[\nabla_s L \nabla_s L^T ] 
\end{equation*}
where $a$ is the input activation vector corresponding to the neural network weight parameters and $s$ is the corresponding preactivation vector,
The natural gradient is then solved by:
\begin{equation}
    s=A^{-1}g \approx F^{-1}g \approx \left(\mathbb{E}[aa^T] \otimes \mathbb{E}[\nabla_s L \nabla_s L^T ] \right)^{-1}g = \mathbb{E}[aa^T]^{-1} g  \mathbb{E}[\nabla_s L \nabla_s L^T ]^{-1}
\end{equation}
The method further speeds up the optimization by reusing recently computed statistic matrices and performing asynchronous computation. It manages to greatly reduce the computation time to be comparable to first-order gradient descent algorithms.
The method manages to perform trust region natural policy gradient optimization computationally efficiently without sacrificing the reinforcement learning performance. A remaining problem is that the method is still batch-based and has a high memory occupation.

\subsection{Proximal Policy Gradient Method}
The author of \cite{schulman2017proximal} proposes a first-order policy optimization method, namely proximal policy optimization (PPO). The method clips the surrogate objective according to the following equation:
\begin{equation}
    L_{CLIP}(\theta) = \mathbb{E} \left[ \min\big(r_t(\theta) A_t , \mathrm{clip}(r_t(\theta),1-\epsilon,1+\epsilon)A_t)\big) \right]
\end{equation}
where $r_t(\theta) = \pi_\theta / \pi_{\theta_{old}}$ is the likelihood ratio of the current policy over the sample policy, and $\epsilon \in [0,1]$ is a hyperparameter. 

The algorithm performs minibatch stochastic optimization at each epoch of collected agent's experience, for a number of passes. Therefore it is actually an off-policy method since the policy has already deviated from the sampling policy once updated for the first minibatch.
%The method can achieve a relatively fast improvement rate in the early phase (within 1 million training steps) of training in several continuous control environments. However, the method is more difficult to tune compared to trust region methods, and might become increasingly unstable in the late phase of training. The method might also converge too early and stuck at a local minimum point, because updating the parameter too many times on a single batch might lead to overfitting.

The advantage of PPO method is that it has both a relatively low time computational complexity and memory complexity. It can achieve a fast convergence rate in terms of the reinforcement learning performance (total episode reward). It also provides some kind of constraint at each epoch so that the policy doesn't deviate too far from the sampling policy. Although the PPO method has a high convergence rate, there is no evidence that shows the method could achieve a better final performance.
\subsection{Other Methods in Deep Reinforcement Learning for Continuous Control}
Several off-policy methods have also been proposed in the problem domain of deep reinforcement learning for continuous control, including the Deep Deterministic Policy Gradient Method (DDPG)~\cite{lillicrap2015continuous}, Actor-Critic with Experience Replay (ACER)~\cite{wang2016sample}, and Trust-PCL~\cite{nachum2017trust}. These methods are off-policy algorithms only utilize first-order information about the policy functions, and are not in the main focus of our study because no evidence has shown that any off-policy methods can significantly outperform the above-mentioned methods in terms of training time, final performance and the stability of performance during training time.
