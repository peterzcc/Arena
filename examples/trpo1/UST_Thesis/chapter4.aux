\relax 
\providecommand\hyper@newdestlabel[2]{}
\citation{henderson2017matters}
\citation{wu2017scalable}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments}{29}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Experiments on the basic source tasks}{29}{section.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Inconsistent performance produced by ACKTR agents with the same parameters but different runs. The vertical axis is the total return averaged over the recent 20 episodes and the horizontal axis is the number of million time-steps}}{30}{figure.4.1}}
\newlabel{fig_acktr_reprod}{{4.1}{30}{Inconsistent performance produced by ACKTR agents with the same parameters but different runs. The vertical axis is the total return averaged over the recent 20 episodes and the horizontal axis is the number of million time-steps}{figure.4.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Performance of ACKTR agents with different KL-divergence constraints. All the agents are trained with batch-size 4000 and 20 parallel agents. The vertical axis is the total return averaged over the recent 200 episodes and the horizontal axis is the number of million time-steps}}{30}{figure.4.2}}
\newlabel{fig_acktr_mom_tune}{{4.2}{30}{Performance of ACKTR agents with different KL-divergence constraints. All the agents are trained with batch-size 4000 and 20 parallel agents. The vertical axis is the total return averaged over the recent 200 episodes and the horizontal axis is the number of million time-steps}{figure.4.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Performance of W-KTR agents with different W2-metric constraints. All the agents are trained with batch-size 4000 and 20 parallel agents. The vertical axis is the total return averaged over the recent 200 episodes and the horizontal axis is the number of million time-steps}}{31}{figure.4.3}}
\newlabel{fig_wass_const_tune}{{4.3}{31}{Performance of W-KTR agents with different W2-metric constraints. All the agents are trained with batch-size 4000 and 20 parallel agents. The vertical axis is the total return averaged over the recent 200 episodes and the horizontal axis is the number of million time-steps}{figure.4.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Performance of a W-KTR agent with a decaying W2-metric from 0.02 to 0.00003 in the first 15 million time-steps. The agent is trained with batch-size 4000 and 20 parallel agents. The vertical axis is the total return averaged over the recent 200 episodes and the horizontal axis is the number of million time-steps}}{32}{figure.4.4}}
\newlabel{fig_wass_decay}{{4.4}{32}{Performance of a W-KTR agent with a decaying W2-metric from 0.02 to 0.00003 in the first 15 million time-steps. The agent is trained with batch-size 4000 and 20 parallel agents. The vertical axis is the total return averaged over the recent 200 episodes and the horizontal axis is the number of million time-steps}{figure.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Experiment on the flat reinforcement learning solution to multi-modality tasks}{32}{section.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.1}Discussion on conventional flat reinforcement learning methods}{32}{subsection.4.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.2}Experiment on exceptional advantage regularization}{33}{subsection.4.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces Performance of agents with different weight on entropy regularization term, the horizontal axis is the number of million timesteps and the vertical axis is the total episode reward averaged over the last 32 episodes}}{34}{figure.4.5}}
\newlabel{rec_ent_reg}{{4.5}{34}{Performance of agents with different weight on entropy regularization term, the horizontal axis is the number of million timesteps and the vertical axis is the total episode reward averaged over the last 32 episodes}{figure.4.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Logarithm of average standard deviation of the policy of agents in Figure\nobreakspace  {}\ref  {rec_ent_reg}, the horizontal axis is the number of million timesteps.}}{35}{figure.4.6}}
\newlabel{rec_std_ent_reg}{{4.6}{35}{Logarithm of average standard deviation of the policy of agents in Figure~\ref {rec_ent_reg}, the horizontal axis is the number of million timesteps}{figure.4.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Performance of ACKTR agent on the "moveg2" task, the horizontal axis is the number of million timesteps and the vertical axis is the total episode reward averaged over the last 20 episodes}}{36}{figure.4.7}}
\newlabel{rec_stat_moveg2}{{4.7}{36}{Performance of ACKTR agent on the "moveg2" task, the horizontal axis is the number of million timesteps and the vertical axis is the total episode reward averaged over the last 20 episodes}{figure.4.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces The avearge reward per time-step of ACKTR agent on the "moveg2" task, the horizontal axis is the number of training batches}}{36}{figure.4.8}}
\newlabel{rec_stat_moveg2_meanrt}{{4.8}{36}{The avearge reward per time-step of ACKTR agent on the "moveg2" task, the horizontal axis is the number of training batches}{figure.4.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces The logarithm of avearge standard deviation of ACKTR agent's policy on the "moveg2" task, the horizontal axis is the number of training batches}}{37}{figure.4.9}}
\newlabel{rec_stat_moveg2_std}{{4.9}{37}{The logarithm of avearge standard deviation of ACKTR agent's policy on the "moveg2" task, the horizontal axis is the number of training batches}{figure.4.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces The distribution of advantage values of the ACKTR agent at batch 0 on the "moveg2" task, the horizontal axis is the log-likelihood value}}{38}{figure.4.10}}
\newlabel{vis_stats_0}{{4.10}{38}{The distribution of advantage values of the ACKTR agent at batch 0 on the "moveg2" task, the horizontal axis is the log-likelihood value}{figure.4.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces The distribution of advantage values of the ACKTR agent at batch 300 on the "moveg2" task, the horizontal axis is the log-likelihood value}}{39}{figure.4.11}}
\newlabel{vis_stats_3000}{{4.11}{39}{The distribution of advantage values of the ACKTR agent at batch 300 on the "moveg2" task, the horizontal axis is the log-likelihood value}{figure.4.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces The distribution of advantage values of the ACKTR agent at batch 4900 on the "moveg2" task, the horizontal axis is the log-likelihood value}}{40}{figure.4.12}}
\newlabel{vis_stats_4900}{{4.12}{40}{The distribution of advantage values of the ACKTR agent at batch 4900 on the "moveg2" task, the horizontal axis is the log-likelihood value}{figure.4.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Performance of agents with different exceptional advantage regularization weights, the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 32 episodes}}{41}{figure.4.13}}
\newlabel{rec_adv_reg}{{4.13}{41}{Performance of agents with different exceptional advantage regularization weights, the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 32 episodes}{figure.4.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2.3}Experiment on the Robust Concentric Mixture Gaussian Policy}{41}{subsection.4.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces The logarithm of avearge standard deviation of agents with different exceptional advantage regularization weights, the horizontal axis is the number of million time-steps and the vertical axis is the total episode reward averaged over the last 32 episodes}}{42}{figure.4.14}}
\newlabel{rec_std_adv_reg}{{4.14}{42}{The logarithm of avearge standard deviation of agents with different exceptional advantage regularization weights, the horizontal axis is the number of million time-steps and the vertical axis is the total episode reward averaged over the last 32 episodes}{figure.4.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Performance of ACKTR agents with different KL-divergence constraints, the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 32 episodes}}{43}{figure.4.15}}
\newlabel{rec_mix}{{4.15}{43}{Performance of ACKTR agents with different KL-divergence constraints, the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 32 episodes}{figure.4.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Hierarchical reinforcement learning methods for multi-modality and sparse environments}{43}{section.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.1}Training the actuator agents with domain randomization by cross-sampling initial states}{44}{subsection.4.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces Performance of actuator agents with domain randomization by cross-sampling initial states, the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 200 episodes}}{45}{figure.4.16}}
\newlabel{rec_8task_training}{{4.16}{45}{Performance of actuator agents with domain randomization by cross-sampling initial states, the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 200 episodes}{figure.4.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Performance of actuator agents using "synchronous scheduling of actuator learning" , the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 200 episodes}}{46}{figure.4.17}}
\newlabel{rec_sync_training}{{4.17}{46}{Performance of actuator agents using "synchronous scheduling of actuator learning" , the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 200 episodes}{figure.4.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3.2}Training the decider agent}{47}{subsection.4.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{Phase 1: Decision policy training}{47}{section*.1}}
\@writefile{toc}{\contentsline {subsubsection}{Phase 2: Switcher policy training}{47}{section*.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Decision policy training performance of the task "dynamicg8", the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 32 episodes}}{48}{figure.4.18}}
\newlabel{fig:rec_dynamicg8_decider_subt10}{{4.18}{48}{Decision policy training performance of the task "dynamicg8", the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 32 episodes}{figure.4.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces Decision policy training performance of the task "reachcont", the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 200 episodes}}{49}{figure.4.19}}
\newlabel{fig:rec_reachc05_decider_subt10}{{4.19}{49}{Decision policy training performance of the task "reachcont", the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 200 episodes}{figure.4.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces Switcher policy training performance of the task "dynamicg8", the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 32 episodes}}{50}{figure.4.20}}
\newlabel{fig:rec_dynamicg8_switcher}{{4.20}{50}{Switcher policy training performance of the task "dynamicg8", the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 32 episodes}{figure.4.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.21}{\ignorespaces Average execution length of the actions of decision policies of the task "dynamicg8", the x-axis is the number of million time-steps and the y-axis is the decision policy's average execution length of the last 2560 timesteps.}}{51}{figure.4.21}}
\newlabel{fig:rec_dynamicg8_avesubt}{{4.21}{51}{Average execution length of the actions of decision policies of the task "dynamicg8", the x-axis is the number of million time-steps and the y-axis is the decision policy's average execution length of the last 2560 timesteps}{figure.4.21}{}}
\@setckpt{chapter4}{
\setcounter{page}{52}
\setcounter{equation}{0}
\setcounter{enumi}{9}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{4}
\setcounter{section}{3}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{21}
\setcounter{table}{0}
\setcounter{NAT@ctr}{0}
\setcounter{Item}{9}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{51}
\setcounter{parentequation}{0}
\setcounter{float@type}{16}
\setcounter{lstnumber}{1}
\setcounter{lofdepth}{1}
\setcounter{lotdepth}{1}
\setcounter{algorithm}{1}
\setcounter{ALG@line}{6}
\setcounter{ALG@rem}{6}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{section@level}{3}
\setcounter{lstlisting}{0}
}
