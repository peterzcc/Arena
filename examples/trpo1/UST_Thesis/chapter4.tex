%!TEX program = xelatex
%!TEX root = ./thesis.tex
\chapter{Experiments}
\input{chapter4_move0.tex}
%\input{chapter4_wass.tex}
\input{chapter4_adv_reg.tex}
\input{chapter4_hrl.tex}
% \section{Preliminary experiment on the hierarchical agent architecture}
% A preliminary experiment has been done on training the proposed agent in the dynamic2d environment. The termination policy sets $b_i = 4.5$ and isn't trained during policy updates. The figure is shown in Figure \ref{rec_180419_fix_ter}.

% The result shows that the learning of the root-level policy is effective.


% \begin{figure}[h]
% \includegraphics[width=\textwidth]{images/rec_180420_fix_ter.pdf}
% \centering
% \caption{Experiment on dynamic2d on a hierarchical reinforcement learning agent with fixed-length termination policy}
% \end{figure}\label{rec_180419_fix_ter}

% The current guess on the source of stability is that the sub-policy doesn't perform well in the target environment. Figure \ref{rec_180419_fix_ter_len} shows the average episode length of the hierachical agent.

% \begin{figure}[h]
% \includegraphics[width=\textwidth]{images/rec_180420_fix_ter_len.pdf}
% \centering
% \caption{The average episode length of the dynamic2d hierarchical agent}
% \end{figure}\label{rec_180419_fix_ter_len}

% \section{Experiment up to April. 26: on transfer of source policies}
% The results show that the source task policy cannot perform well in dynamic2d, because of different initial condition caused by the switching of moving direction.

% An experiment is developed to fine-tune the source task policies during the learning of the target task. However, the problem is on how to fine-tune a Gaussian policy that has already converged with low STD.

% \section{Ongoing development in transfer of actuator policy}
% \section{On training source tasks}
% I found that using a 1-step proximate policy gradient algorithm with an adaptive KL penalty is the most economic method.

% Previously, the ACKTR agent sometimes stuck at a performance around 3000 as discussed in the group meeting in March. The reason could be due to KL divergence constraint is too small. A small change in the mean vector could lead to a large KL divergence when the STD converges to nearly zero. That prevents the agent's policy from improving.

% It is likely that an alternative metric could be better for trust region methods, like L1-error or JS-divergence.
% \subsection{Training the actuator policy from scratch in the target task}
% An experiment is being run to test if the agent can effectively learn the actuator policies. The actuator agent learns stably. However the rate of improvement is slow due to the simultaneous training of all the source tasks. The current per step mean reward reaches 1.5 after 3 days.
% \subsection{Fine-tuning the actuator policies}
% The fine-tuning of the actuator polices seems to fail in the target problem. The performance usually drops during the fine-tuning period. 
% Deeper investigations are undergoing to fix the problem.
% \subsection{Training a transition policy}
% The development of the transition policy is still undergoing.
% \section{On the hierarchical reinforcement learning}
% I found that the current scheduler agent model is unstable in training. 

% I'm trying to develop a mixed type policy network, where the output is a binary variable and the discrete/continuous distribution. The binary variable indicates whether the current actuator policy should be terminated, and the latter one denotes the policy distribution. The relevant properties such as KL divergence for this kind of distribution should be derived and developed.

% \section{Progress by May. 10}
% Current work still focuses on the investigation of transferring actuator policies.
% The current model fails to reproduce the previous positive experiments on multi-modality tasks. I've done several bug-fixes and parameter tuning, however the problem still remains. The cause is still under investigation, one possible cause is related to the stability of actuator policies in terms of failure rate. If an actuator policy is prune to "game over" behaviour when initialized after another actuator policy, the decider agent may tend to constantly choose a fixed policy. 

% \section{Progress by Mar. 17}
% \subsection{Reproduction of the multi-modality environment performance}
% I've just finished fixing the code, so that the new model achieves a reasonable performance in move1d. The performance is shown in Figure \ref{rec_move1d}.
% \begin{figure}[h]
% \includegraphics[width=\textwidth]{images/rec_180517_move1d_better.pdf}
% \centering
% \caption{Experiment on move1d}
% \end{figure}\label{rec_move1d}

% \subsection{Study of generalized advantage estimation}
% During the development of the hierarchical reinforcement learning model, I developed a modified version of \cite{schulman2015high} for hierarchical reinforcement learning methods. I found a problem that the original method didn't correctly normalize the advantage values $A_t$ according to $\sum_{i=t}^{i=t+T}\lambda^{i}$. That would lead to a advantage values linearly increasing magnitudes with the increased time to termination. However, I found that the correct method doesn't produce a better result according to the experiments up to now.

% \subsection{Policy gradient methods with Wasserstein metric}
% I found that one problem with the current KL-divergence based trust region methods is that agents will find it difficult to learn after the STD of the Gaussian policy approaches zero, due to the sensitive KL-divergence loss. I found that it seems better to replace the KL-divergence metric with Wasserstein metric for Gaussian policies.

% A preliminary experiment is done on the original Ant task and has shown positive results, which is shown in Figure \ref{rec_wass}. The proposed agent is the policy gradient agent with adaptive Wasserstein metric penalty.

% \begin{figure}[h]
% \includegraphics[width=\textwidth]{images/rec_180517_wasserstein.pdf}
% \centering
% \caption{Experiment comparison on Wasserstein metric and KL divergence. agent\_1 is the Wasserstein agent}

% \end{figure}\label{rec_wass}

