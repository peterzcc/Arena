%!TEX program = xelatex
%!TEX root = ./thesis.tex

\chapter{Introduction}
\section{Overview}
Deep reinforcement learning methods have become popular in the machine learning field in the recent years. Through the combination of deep neural network models and reinforcement learning theory, advances in deep reinforcement learning have achieved human level control performance in playing Atari games \cite{mnih2015human}, mastered the board game GO \cite{silver2016mastering}, learned to play Texas Hold'em pokers \cite{heinrich2016deep}, and learned to solve simple robot control tasks \cite{duan2016benchmarking}.

However, the capability of reinforcement learning is still very limited. The end-to-end deep reinforcement learning methods are not able to solve general real-world challenging problems, such as playing the StarCraft games, controlling industrial assembly robots and making stock trading decisions. The contemporary end-to-end deep reinforcement learning methods have been able to handle simple tasks with unknown environment dynamics, or relatively complex environments that have known models. The real-world problems, however, usually have uncertain dynamics, noisy observation space and relative complex task logics. The study on improving the capability of deep reinforcement learning is thus still an important issue.

In this study, we investigate a set of challenging robot control environments and propose several methods that can solve the tasks.
%TODO polish
\section{Background}

\subsection{Reinforcement Learning (RL)}
Reinforcement learning~\cite{sutton1998reinforcement} is learning a policy (map from situations to actions) in order to maximize a numerical reward signal. The learner must discover the optimal policy by trying them. Reinforcement learning is different from supervised learning that the learner is not provided any labelled examples by any knowledgeable external supervisor. Reinforcement learning is also different from unsupervised learning, whose objective is finding hidden structures in unlabeled data.

Most reinforcement learning study is based on the problem formulation of Markov Decision Processes (MDPs). MDP is a framework defining the problem of learning from interaction. The learner is called the \textit{agent}. Everything outside that interacts with the agent is called the \textit{environment}.
The agent and environments interact during a finite sequence of discrete time steps, $t=0,1,2,3,...$. At each time step t, the agent receives an observation of the environment's \textit{state}, $s \in S$ and selects an \textit{action} $a \in A$. The agent receives a numerical reward $r$, and also the next state $s^\prime$.  A finite-horizon discounted Markov decision process (MDP) is defined by the tuple $(S,A,P,r,\rho_0,\gamma) $ where the $\rho_0 : S \mapsto \mathbb{R}$ is the initial state distribution, $P(s^\prime|s,a) : S \times A \mapsto \mathbb{R}$ is the distribution describing the state transition dynamics, $r : S \times A \mapsto \mathbb{R}$ is the expected reward function, and $\gamma \in (0,1]$ is the discount factor.

Let $\pi : S \times A \mapsto [0,1] $ denote a policy, following are definitions of the action value function $Q_\pi $, the value function $V_\pi $ and the advantage function $A_\pi $:
$$ Q_\pi(s_t,a_t) = \mathbb{E}_{s_{t+1},a_{t+1},\ldots}
\big[ \sum_{l=0}^\infty r(s_{t}) \big],$$
$$ V_\pi(s_t) = \mathbb{E}_{a_{t},s_{t+1},a_{t+1},\ldots}
\big[ \sum_{l=0}^\infty r(s_{t}) \big],$$

$$ A_\pi (s,a) = Q_\pi (s,a) - V_\pi (s), \text{where } a_t \sim \pi (a_t|s_t), \text{for } t \geq 0 $$

%%TODO: think about where I can put this section
%\subsection{Hierarchical reinforcement learning based on transferred policies from a set of source tasks}\label{sec_hier_trans}



\section{Scope of study}
Real-world robot control problems involves many sources of complexity. However, we only focus on some critical properties of real-world robot control problems.
This study focus on the reinforcement learning problems with multi-modality state space, continuous action space and sparse reward in a continual learning setting.

\subsection{Multi-modality state space}

The reinforcement learning problems with multi-modality state space refers to the environments whose state space is a combination of signals with different modality.

Specifically, the study focus on realistic robot control environments whose state space is a combination of a low-dimensional state sensor and a image sensor observation.

The low-dimensional state input provides accurate information about the agent's physical system. The image input provides information about the external environment and has a datatype that is in much higher dimension.

Most recent works in reinforcement learning for continuous control focus on the RL tasks with low-dimensional state input only~\cite{duan2016benchmarking}. There are also works ~\cite{wu2017scalable} trying to handle some tasks with image input only, but these image-based tasks are very simple to solve.

However, the proposed multi-modality environment is the most common case in real-world robotic problems, and is considerably more complex and realistic than solving a low-dimensional state input only task or an image only task.


\subsection{Sparse reward function}
The problem of sparse / delayed reward signal has been one of the major problems to be tackled in reinforcement learning. Many methods are proposed to solve the sparse reward problem in environments with discrete action spaces. However, the problem is much more difficult for the environments with continuous action spaces, and few works have focused on the class of system. By the time of writing, we haven't seen any method that has been proposed to effectively solve continuous control tasks with sparse rewards.

Studies have shown that smooth and informative reward function can generally make the training of reinforcement learning agents easier. Theoretically, any reinforcement learning problems when given a reward function that is good enough.

However, the reward function of a realistic robot control environment is usually discontinuous and sparse. Reward shaping techniques may improve the reward function, but it usually relies on domain-specific engineering. In our study, we try to solve some realistic robot problems without reward shaping, which essentially indicates the problems have sparse reward functions.


\subsection{Assumptions on relationship of tasks}

We are particularly interested in a class of reinforcement learning problems, where the objective is to solve a \textit{target task } given a set of trained \textit{source tasks}. The policies that can solve the source tasks, namely the \textit{actuator policies}, are available for the agent. We assume that the target task can be solved by a sequential execution of a subset of the source-task policies. An \textit{decider agent} needs to be trained, which chooses the source-task policy to be executed and decides when to switch between them.

This paradigm is related to hierarchical reinforcement learning (HRL), which focuses on learning and utilizing closed-loop partial policy / skills to solve a complex task. However, our case is different from the standard setting of HRL where the agent needs to extract the useful closed-loop partial policy from the original task. However, we focus on extracting and reusing partial policies from other tasks.

This paradigm is also related to transfer learning in reinforcement learning~\cite{taylor2009transfer}, because the source-task policies are reused by the high-level scheduler agent. However, conventional transfer learning paradigms usually don't assume a hierarchical relationship between the target task and the source tasks.

This is the only assumption on the relationship between the source tasks and the target task. It is not known whether any specific source task is useful. A source task policy might be useful in some cases, or it could be completely useless. The minimum execution length of an actuator policy is also unknown. An actuator policy might be executed for only 1 time-step before the agent switch to another one, or it could be executed for the whole episode.

This assumption needs domain-specific design on the selection of source tasks. However, it is common that real-world complex robot control problems can be decomposed to primary tasks. For example, classical robot locomotion methods organize the abstract problems (such as trajectory planning) and low level problems (such as motor control) in hierarchical structures.

If this hierarchical task relationship assumption was removed, the problem becomes the fully autonomous hierarchical reinforcement learning problem, where the agents need to learn not only a task structure but also all actuator policies from scratch. This problem is currently considered in-feasible to solve for general robot control problems~\cite{barto2003recent}.


\section{Research Questions}
% TODO: there should be a larger set of task settings, to prove that the method is effective for general tasks.
% an have the same set of source tasks with different target tasks
% The questions should be specific that the experiment results and comparison can show the answer clearly, with measurable quantity
% Specific performance metric
% just define the proposed tasks and reference it to the later section, state that the tasks are more realistic


We will propose several reinforcement learning environments for continuous control in section \ref{sec_env} according to the stated scope and assumption. The study will mainly focus on solving the proposed problems.
The primary research question that we focus on is:
\begin{center}
    \textit{Can we develop an agent that can achieve good performance (in terms of total reward) in the reinforcement learning environments defined in section \ref{sec_env} within reasonable training time, with an RL model that doesn't rely on domain-specific techniques  (including reward shaping, manual definition of policy hierarchy, manual / heuristic methods to define sub-goals, or manual criteria on the switching of actuator policies) given a predefined set of source tasks?}
\end{center}

We will first answer the question on whether the previous work already solve the problem: \textit{Can the contemporary flat reinforcement learning methods solve the proposed environments?}


We will then answer the question on whether an end-to-end hierarchical reinforcement learning can solve this:
A model that solves the primary question needs to address two issues: reusing the policies learned in source tasks and solving the target task.

To solve the first issue, we need to answer the question of "\textit{Can a trained actuator policy achieve an acceptable performance (in terms of the expected total return of the original source task), with or without fine-tuning methods, in a target environment, which usually has different initial state distribution?}".

To address the second issue, we will answer the question of "\textit{Can a hierarchical reinforcement learning agent be developed to solve the target tasks defined in \ref{sec_env} given the trained actuators policies, without other domain-specific designs?}".

% \section{Thesis Outline and Contributions}
% The remainder of this thesis is structured as follows. TODO