%!TEX program = xelatex
%!TEX root = ./thesis.tex
\chapter{Introduction}
\section{Overview}
% TODO: elaborate
Deep reinforcement learning methods have gained much attention from researchers and the public since the recent advances such as DQN~\cite{mnih2015human} and AlphaGo~\cite{silver2016mastering}. The method of deep reinforcement learning combine deep neural network models and reinforcement learning algorithms, enabling the potential for solving general control problems in fully autonomous manner. 
However, the application of contemporary deep reinforcement learning methods are still limited to a finite class of problems. The reinforcement learning problem either has a fully known model like the Go game, or has to be problems with low complexity like Atari game \cite{mnih2015human} or primary robot control tasks \cite{duan2016benchmarking}. The real-world robot control problems are far from being solved without labour-intensive domain specific engineering.

In this study, we propose a hierarchical reinforcement learning method that can learn to solve increasingly complex tasks in several realistic robot control environments.

\section{Background}
% TODO: finish
\subsection{Reinforcement Learning (RL)}
Reinforcement learning~\cite{sutton1998reinforcement} is learning a policy (map from situations to actions) in order to maximize a numerical reward signal. The learner must discover the optimal policy by trying them. Reinforcement learning is different from supervised learning that the learner is not provided any labelled examples by any knowledgeable external supervisor. Reinforcement learning is also different from unsupervised learning, whose objective is finding hidden structures in unlabeled data.

Most reinforcement learning study is based on the problem formulation of Markov Decision Processes (MDPs). MDP is a framework defining the problem of learning from interaction. The learner is called the \textit{agent}. Everything outside that interacts with the agent is called the \textit{environment}.
The agent and environments interact during a finite sequence of discrete time steps, $t=0,1,2,3,...$. At each time step t, the agent receives an observation of the environment's \textit{state}, $s \in S$ and selects an \textit{action} $a \in A$. The agent receives a numerical reward $r$, and also the next state $s^\prime$.  A finite-horizon discounted Markov decision process (MDP) is defined by the tuple $(S,A,P,r,\rho_0,\gamma) $ where the $\rho_0 : S \mapsto \mathbb{R}$ is the initial state distribution, $P(s^\prime|s,a) : S \times A \mapsto \mathbb{R}$ is the distribution describing the state transition dynamics, $r : S \times A \mapsto \mathbb{R}$ is the expected reward function, and $\gamma \in (0,1]$ is the discount factor.

Let $\pi : S \times A \mapsto [0,1] $ denote a policy, following are definitions of the action value function $Q_\pi $, the value function $V_\pi $ and the advantage function $A_\pi $:
$$ Q_\pi(s_t,a_t) = \mathbb{E}_{s_{t+1},a_{t+1},\ldots}
\big[ \sum_{l=0}^\infty r(s_{t}) \big],$$
$$ V_\pi(s_t) = \mathbb{E}_{a_{t},s_{t+1},a_{t+1},\ldots}
\big[ \sum_{l=0}^\infty r(s_{t}) \big],$$

$$ A_\pi (s,a) = Q_\pi (s,a) - V_\pi (s), \text{where } a_t \sim \pi (a_t|s_t), \text{for } t \geq 0 $$


\subsection{Hierarchical reinforcement learning based on transferred policies from easy tasks}\label{sec_hier_trans}

This study focus on a class of reinforcement learning problems, where the objective is to solve a \textit{target task } given a set of trained \textit{source tasks}. The policies that can solve the source tasks, namely the \textit{actuator policies}, are available for the agent. It is known that target task can be solved by a sequential execution of a subset of the source-task policies. The agent needs to learn a \textit{scheduler policy}, which chooses the source-task policy to be executed and decides when to switch the currently running source-task policy.

This paradigm is related to hierarchical reinforcement learning (HRL)~\cite{barto2003recent}, which focuses on learning and utilizing closed-loop partial policy / skills to solve a complex task. However, our case is different from the standard setting of HRL where the agent needs to extract the useful closed-loop partial policy from the original task. However, we focus on extracting and reusing partial policies from other tasks.

This paradigm is also related to transfer learning in reinforcement learning~\cite{taylor2009transfer}, because the source-task policies are reused by the high-level scheduler agent. However, conventional transfer learning paradigms usually don't assume a hierarchical relationship between the target task and the source tasks.



\section{Scope of study}
Real-world robot control problems involves many sources of complexity. However, we only focus on some critical properties of real-world robot control problems.
This study focus on the reinforcement learning problems with multi-modality state space, continuous action space and sparse reward in a continual learning setting.

\subsection{Multi-modality state space}

The reinforcement learning problems with multi-modality state space refers to the environments whose state space is a combination of signals with different modality.

Specifically, the study focus on realistic robot control environments whose state space is a combination of a low-dimensional state sensor and a image sensor observation.

The low-dimensional state input provides accurate information about the agent's physical system. The image input provides information about the external environment and has a datatype that is in much higher dimension.

Most recent works in reinforcement learning for continuous control focus on the RL tasks with low-dimensional state input only~\cite{duan2016benchmarking}. There are also works ~\cite{wu2017scalable} trying to handle some tasks with image input only, but these image-based tasks are very simple to solve.

However, the proposed multi-modality environment is the most common case in real-world robotic problems, and is considerably more complex and realistic than solving a low-dimensional state input only task or an image only task.


\subsection{Sparse reward function}
The problem of sparse / delayed reward signal has been one of the major problems to be tackled in reinforcement learning. Many methods are proposed to solve the sparse reward problem in environments with discrete action spaces. However, the problem is much more difficult for the environments with continuous action spaces, and few works have focused on the class of system. By the time of writing, we haven't seen any method that has been proposed to effectively solve continuous control tasks with sparse rewards.

Studies have shown that smooth and informative reward function can generally make the training of reinforcement learning agents easier. Theoretically, any reinforcement learning problems when given a reward function that is good enough.

However, the reward function of a realistic robot control environment is usually discontinuous and sparse. Reward shaping techniques may improve the reward function, but it usually relies on domain-specific engineering. In our study, we try to solve some realistic robot problems without reward shaping, which essentially indicates the problems have sparse reward functions.


\subsection{Assumption on hierarchical relationship of tasks}
We focus on solving the reinforcement learning setting described in section \ref{sec_hier_trans} . We assume that the agent has already learned the optimal policies for a set of source tasks before solving the target problem, and we also assume that the target task can be solved by executing a subset of the source task policies in according to some "high level policy".

However, we don't pose any other assumption on the relationship between the source tasks and the target task. We don't know whether any specific source task is useful. A source task policy might be useful in some cases, or it could be completely useless. 
We also don't make assumption on how long an actuator policy should be executed in its term. The scheduler agent need to learn it instead. An actuator policy might be executed for only 1 time-step before the agent switch to another one, or it could be executed for a longer time, or until a specific condition.

This assumption is the only assumption that is involved in domain-specific design. However, we observe that many real-world complex robot control problems can usually be decomposed to primary tasks. Classical robot control methods also organize the abstract problems (such as trajectory planning) and low level problems (such as model control) in hierarchical structures.

If this hierarchical task relationship assumption was removed, the problem becomes the fully autonomous hierarchical reinforcement learning problem, where the agents need to learn not only a task structure but also all actuator policies from scratch. This problem is considered in-feasible to solve for general robot control problems~\cite{barto2003recent}.


\section{Research Questions}
% TODO: there should be a larger set of task settings, to prove that the method is effective for general tasks.
% an have the same set of source tasks with different target tasks
% The questions should be specific that the experiment results and comparison can show the answer clearly, with measurable quantity
% Specific performance metric
% just define the proposed tasks and reference it to the later section, state that the tasks are more realistic
%TODO: more precisely

We will propose several reinforcement learning environments for continuous control in section \ref{sec_env} according to the stated scope and assumption. The study will mainly focus on solving the proposed problems.
The primary research question that we focus on is:
\begin{center}
    \textit{Can we develop an agent that can achieve good performance (in terms of total reward) in the reinforcement learning environments defined in section \ref{sec_env} within reasonable training time, with an RL model that doesn't rely on domain-specific techniques (including reward shaping, manual definition of policy hierarchy, manual / heuristic methods to define sub-goals, or manual criteria on the switching of actuator policies)?}
\end{center}

We will first answer the question on whether the previous work already solve the problem: \textit{Can the contemporary flat reinforcement learning methods achieve good performance in the proposed environments?}


We will then answer the question on whether our proposed model can solve this:
A model that solves the primary question needs to address two issues: reusing the policies learned in source tasks and solving the target task.
% TODO First issue can policies for the source perform well
To solve the first issue, we need to answer the question of "\textit{Can a trained actuator policy achieve a good performance (in terms of the expected total return of the original problem), with or without fine-tuning methods, in a target environment which has different initial state distribution?}".

To address the second issue, we will answer the question of "\textit{Can the target scheduler agent learn the optimal policy given the trained sub-task policies, without any domain-specific design defined previously?}".

% \section{Thesis Outline and Contributions}
% The remainder of this thesis is structured as follows. TODO