%!TEX program = xelatex
%!TEX root = ./thesis.tex

\chapter{Introduction}
\section{Overview}
Deep reinforcement learning methods have become popular in the machine learning field in the recent years. Through the combination of deep neural network models and reinforcement learning theory, advances in deep reinforcement learning have achieved human level control performance in playing Atari games \cite{mnih2015human}, mastered the board game GO \cite{silver2016mastering}, learned to play Texas Hold'em pokers \cite{heinrich2016deep}, to solve simple robot control tasks \cite{duan2016benchmarking} and to play a simplified version of the game Dota 2~\cite{openai_2018}.

However, the capability of contemporary deep reinforcement learning methods is still limited when we consider real-world problems, such as playing the StarCraft games, controlling robots to do meaningful tasks and making stock trading decisions. Generally speaking, the contemporary end-to-end deep reinforcement learning methods have been able to solve two classes of problems. The first class of problems consists of single-agent environments with unknown state-transition dynamics but trivial task logics, such as the Atari games. The second class of problems consists of the multi-agent games which could have relatively complex task logics but in a closed system, such as the board game Go. 

General real-world robot problems, however, are more challenging. Compared with video game environments like~\cite{mnih2015human}, robot problems have continuous action spaces instead of discrete spaces. This has made the policy exploration problem much more difficult. Apart from that, most robot control problems have multi-modal state space consisting both low-dimensional and high-dimensional inputs. The multi-modality property further introduces complexity to reinforcement learning problems because the local-minimum problem is much more difficult than in supervised learning domains. Finally, the lack of smooth and informative reward signals in realistic robot problems pose significant challenges to contemporary end-to-end deep reinforcement learning methods.

In this study, we investigate a set of challenging robot control environments and propose flat and hierarchical reinforcement learning methods to solve them.
%TODO elaborate
\section{Background in Reinforcement Learning (RL)}
Reinforcement learning~\cite{sutton1998reinforcement} is the problem of learning a policy that map from situations to actions in order to maximize a numerical reward signal. The learner must discover the optimal policy by examining different possibilities. Reinforcement learning is different from supervised learning that the learner is not provided labeled examples by a knowledgeable external supervisor. Apart from that, reinforcement learning agents are trained on the data that are experiences generated by itself, instead of any existing training data. Therefore, reinforcement learning agents not only need to exploit their experience, but also need to generate experience that efficiently explore the problem space.  Reinforcement learning is also different from unsupervised learning, whose objective is finding hidden structures in unlabeled data without any guiding signal.

Most reinforcement learning study is based on the problem formulation of Markov Decision Processes (MDPs). MDP is a framework defining the problem of learning from interaction. The learner is called the \textit{agent}. Everything outside that interacts with the agent is called the \textit{environment}.
The agent and environments interact during a finite sequence of discrete time steps, $t=0,1,2,3,...$. At each time step t, the agent receives an observation of the environment's \textit{state}, $s \in S$ and selects an \textit{action} $a \in A$. The agent receives a numerical reward $r$, and also the next state $s^\prime$.  A finite-horizon discounted Markov decision process (MDP) is defined by the tuple $(S,A,P,r,\rho_0,\gamma) $ where the $\rho_0 : S \mapsto \mathbb{R}$ is the initial state distribution, $P(s^\prime|s,a) : S \times A \mapsto \mathbb{R}$ is the distribution describing the state transition dynamics, $r : S \times A \mapsto \mathbb{R}$ is the expected reward function, and $\gamma \in (0,1]$ is the discount factor.

Let $\pi : S \times A \mapsto [0,1] $ denote a policy, following are definitions of the action-value function $Q_\pi $, the value function $V_\pi $ and the advantage function $A_\pi $:
\begin{align}
Q_\pi(s_t,a) &= \mathbb{E}_{s_{t+1},a_{t+1},\ldots}
\big[ \sum_{l=0}^\infty \gamma^l r(s_{t}) \big], \\
V_\pi(s_t) &= \mathbb{E}_{a_{t},s_{t+1},a_{t+1},\ldots}
\big[ \sum_{l=0}^\infty \gamma^l  r(s_{t}) \big],\\
& \ \ \ \ \text{where } a_t \sim \pi (a_t|s_t), \text{for } t \geq 0  \\
A_\pi (s,a) &= Q_\pi (s,a) - V_\pi (s), 
\end{align}
The action-value function $Q$ estimates the average total discounted reward following the policy $\pi$ after taking the action $a$ at the current state $s_t$. The value function $V$ estimates the average total discounted reward following the policy $\pi$ both at the current state and in the future. The advantage function $A$ is the difference between $Q$ and $V$, which indicates the advantage of taking the action $a$ at the current state over the policy $\pi$.

%%TODO: think about where I can put this section
%\subsection{Hierarchical reinforcement learning based on transferred policies from a set of source tasks}\label{sec_hier_trans}



\section{Scope of Study}
Realistic robot control problems in reinforcement learning involve many sources of complexity. However, we are only concerned with some of the typical problems.
We focus on a set of reinforcement learning problems with multi-modality state space and continuous action space. We also specifically study some problems with not only the previously mentioned properties, but also with sparse reward functions.

\subsection{Multi-modality State Space}

The state spaces of a vast majority of the problems in previous reinforcement learning studies are in single-modality, such as a low-dimensional vector of internal game state or locomotion sensor signal, or a single high dimensional image stream. Most recent works in reinforcement learning for continuous control focus on the RL tasks with low-dimensional state input only~\cite{duan2016benchmarking}. Some study~\cite{wu2017scalable} also tried to handle tasks with a single image-input but in limited complexity.

However, it is more common in real-world environments that a robot agent has multiple types of sensors installed simultaneously. In such case, the state space of the reinforcement learning problem is a combination of input with different modalities. The different modalities in the state space could provide complementary information that cannot be substituted. For a typical robot locomotion problem, the low-dimensional motion sensor input usually provides accurate information about the agent's physical status. The image input, on the other hand, may provide information on the external environments and object relationships. If the motion sensor input is missing, the robot agent would not perform its motor control well, on the other hand, if the image sensor is missing, the robot agent would not know anything about its task and environment.


Apart from motion sensor and image sensor, real-world robot agents may have other modalities such as wireless network antenna input, human language instructions and laser sensor. This thesis focus on robot control environments whose state observation consists of a low-dimensional motion sensor signal and an image sensor stream, which is among the most typical cases of real-world robot problems.

In the context of reinforcement learning, the previously described multi-modality problem is significantly more complex than single-modality problems. The reason is that the feature of image observations is much more difficult to learn than the motion sensor signals. End-to-end reinforcement learning methods would typically prioritize the low-dimensional input in the initial phase of training, and get stuck until the agent is able to extract useful information from the image input. The local-minimum problem in this case is more challenging than the cases in supervised learning, because the generation of training samples are also controlled by the agent's acting policy.

\subsection{Sparse Reward Function}
The sparse / delayed reward signal problem has been one of the major problems in reinforcement learning domains for the past 20 years. The reinforcement learning problems with sparse rewards refer to the environments where informative reward signals are very rare. For example, the agent in such environments may just receive a constant negative reward at most timesteps but only receives a positive reward when some specific rare event happens. The agent not be able to learn anything about a good policy if it has never triggered such events that generate reward signal. Apart from that, the agent may not be able to improve further after experienced such events if it has already been very conservative in the degree of exploration. The core problem is how the agent can perform efficient exploration in the environment.

Many methods have been proposed to solve the sparse reward problem in environments. However, none of the methods are able to solve the general case. Some of them could only solve special cases of reinforcement learning problems, that are far from practical problems, while the others require a tremendous amount of domain-specific engineering component.

A smooth and informative reward function can generally make the training of reinforcement learning agents easier. Theoretically, any reinforcement learning problems when given a reward function that is informative enough could be solved easily by contemporary methods. However, such a reward function itself is rarely accessible in real-world environments. Reward shaping~\cite{ng1999policy} techniques may be used to improve the reward function, but such methods usually also relies on domain-specific engineering. In our study, we try to solve several realistic sparse environments without the application of reward shaping.


\subsection{Assumptions on Relationship of Tasks}
Apart from the objective to solve sparse multi-modal reinforcement learning problems with end-to-end reinforcement learning problems, we are also particularly interested in a paradigm, where the objective is to solve a \textit{target task } given a set of \textit{source tasks}. The policies that solve the source tasks, namely the \textit{actuator policies}, are available to the agent when it is faced with the target task. We call them actuator policies because these policies perform low-level control skills instead of making the highest level decision in the target task. We assume that the target task can be solved by a sequential execution of a subset of the actuator policies. An \textit{decider agent} needs to be trained, which controls the scheduling of the actuator policies.

This paradigm is related to hierarchical reinforcement learning (HRL), which focuses on learning and utilizing closed-loop partial policy / skills to solve a complex task. However, our case is different from the standard setting of HRL. The standard HRL setting assumes that the agent needs to extract the useful closed-loop partial policy from the original target task. However, we focus on extracting and reusing partial policies from other tasks.

This paradigm is also related to transfer learning in reinforcement learning~\cite{taylor2009transfer}, because the source-task policies are reused by the decider agent to solve another task. However, conventional transfer learning paradigms usually don't pose assumptions on a hierarchical relationship between the target task and the source tasks.

This is the only assumption on the relationship between the source tasks and the target task. It is not known whether any specific source task is useful. A source task policy might be useful in some cases, or it could be completely useless. The execution time of an actuator policy is also unknown. An actuator policy might be executed for only 1 time-step before the agent switch to another one, or it might also be executed for the whole episode.

Since the set of source-task policies is pre-defined and there is no autonomous method in selecting then, the selection of the source tasks is considered a domain-specific design component. However, it is common that real-world complex problems can be decomposed to primary tasks. For example, classical robot locomotion methods organize the abstract problems (such as trajectory planning) and low level problems (such as motor control) in hierarchical structures. Apart from that, it is also common that human beings would decompose the original problem into a series of easier problems when the problem it faced is too complex.

If this hierarchical task relationship assumption was not present, the problem would belong to the class of the fully autonomous hierarchical reinforcement learning problem. In this case, the agents need to learn not only a task structure but also all actuator policies from scratch. The solution of such problem is currently considered in-feasible to obtain for general cases~\cite{barto2003recent}.


\section{Research Questions}
% TODO: there should be a larger set of task settings, to prove that the method is effective for general tasks.
% an have the same set of source tasks with different target tasks
% The questions should be specific that the experiment results and comparison can show the answer clearly, with measurable quantity
% Specific performance metric
% just define the proposed tasks and reference it to the later section, state that the tasks are more realistic


We will propose several reinforcement learning environments for continuous control in section \ref{sec_env} according to the stated scope, and the proposed problems will be of the main focus of the study.
The primary research question that we would like to study is:

    \textit{Can we develop an agent that can achieve sufficient performance (in terms of total episode reward) in the reinforcement learning environments defined in section \ref{sec_env} within a feasible training time, with an RL model that doesn't rely on domain-specific techniques  (including reward shaping, manual definition of policy hierarchy, manual / heuristic methods to define sub-goals, or manual criteria on the switching of actuator policies), with or without a predefined set of source tasks?}


We will first answer the question on whether a hierarchical architecture is necessary: 

\textbf{Research Question 1: }\textit{Can a contemporary flat reinforcement learning method solve the proposed environments?}


We will then answer the question on whether an end-to-end hierarchical reinforcement learning can solve this:
A model that solves the question needs to address two issues: reusing the policies learned in source tasks and solving the target task. The two issues lead to the following two research questions respectively:

\textbf{Research Question 2.1: }\textit{Can a trained actuator policy achieve an acceptable performance (in terms of the expected total return of the original source task) in a target environment, which might have different initial state distribution?}.

\textbf{Research Question 2.1: }\textit{Can a decider agent be developed to solve the target tasks defined in \ref{sec_env} given the trained actuators policies, without other domain-specific designs?}.

% \section{Thesis Outline and Contributions}
% The remainder of this thesis is structured as follows. TODO