%!TEX program = xelatex
%!TEX root = ./thesis.tex

\chapter{Introduction}
\section{Overview}
Deep reinforcement learning methods have become popular among machine learning researchers in the recent years. Through the combination of deep neural network models and reinforcement learning theory, recent works have managed to solve several Atari games \cite{mnih2015human}, the board game GO \cite{silver2016mastering}, Texas Hold'em pokers \cite{heinrich2016deep}, several simple robot control tasks \cite{duan2016benchmarking} and a simplified version of the game Dota 2~\cite{openai_2018}.

However, the capability of contemporary deep reinforcement learning methods is still limited when faced with real-world problems, such as playing the StarCraft games and controlling robots to navigate autonomously. Generally speaking, the contemporary end-to-end deep reinforcement learning methods have been able to solve two classes of problems. The first class of problems are single-agent environments with unknown state-transition dynamics but trivial task logics, such as the Atari games. The second class of problems consists of the multi-agent games which could have relatively complex task logics but a closed system, such as the board game Go and the Dota 2 version in~\cite{openai_2018}. 

General real-world robot problems, however, are much more challenging. Compared with video game environments like Atari~\cite{mnih2015human}, the action spaces of robot problems are continuous. This has made the policy exploration problem much more difficult. Apart from that, many robot control problems have multi-modal state space. This property further introduces complexity to reinforcement learning problems because the local-minimum problem is much more difficult to solve here than in supervised learning domains. Furthermore, the lack of smooth and informative reward signals pose significant challenges to contemporary flat deep reinforcement learning methods.

In this study, we study a set of challenging robot control environments and propose several flat and hierarchical reinforcement learning methods to solve them.
\section{Background in Reinforcement Learning (RL)}
Reinforcement learning~\cite{sutton1998reinforcement} is the problem of learning a policy that maps from situations to actions in order to maximize a numerical reward signal. The learner must discover the optimal policy by examining different possibilities. Reinforcement learning is different from supervised learning that the learner is not provided labeled examples by a knowledgeable external supervisor. A reinforcement learning agent can only learn from the experiences generated by itself, instead of any existing training data. Therefore, reinforcement learning agents have to not only learn from their experience, but also efficiently explore the problem space so that the optimal solution is found.  Reinforcement learning is also different from unsupervised learning, whose objective is finding hidden structures in existing unlabeled data.

Most reinforcement learning study is based on the problem formulation of Markov Decision Processes (MDPs). MDP is a framework defining the problem of learning from interaction. The learner is called the \textit{agent}. Everything outside that interacts with the agent is called the \textit{environment}.
The agent and environments interact during a finite sequence of discrete time steps, $t=0,1,2,3,...$. At each time step t, the agent receives an observation of the environment's \textit{state}, $s \in S$ and selects an \textit{action} $a \in A$. The agent receives a numerical reward $r$, and also the next state $s^\prime$.  A finite-horizon discounted Markov decision process (MDP) is defined by the tuple $(S,A,P,r,\rho_0,\gamma) $ where the $\rho_0 : S \mapsto \mathbb{R}$ is the initial state distribution, $P(s^\prime|s,a) : S \times A \mapsto \mathbb{R}$ is the distribution describing the state transition dynamics, $r : S \times A \mapsto \mathbb{R}$ is the expected reward function, and $\gamma \in (0,1]$ is the discount factor.

Let $\pi : S \times A \mapsto [0,1] $ denote a policy, the following equations are definitions of the action-value function $Q_\pi $, the value function $V_\pi $ and the advantage function $A_\pi $:
\begin{align}
Q_\pi(s_t,a) &= \mathbb{E}_{s_{t+1},a_{t+1},\ldots}
\big[ \sum_{l=0}^\infty \gamma^l r(s_{t+l},a_{t+l}) \big], \\
V_\pi(s_t) &= \mathbb{E}_{a_{t},s_{t+1},a_{t+1},\ldots}
\big[ \sum_{l=0}^\infty \gamma^l  r(s_{t+l},a_{t+l}) \big],\\
& \ \ \ \ \text{where } a_t \sim \pi (a_t|s_t), \text{for } t \geq 0  \\
A_\pi (s,a) &= Q_\pi (s,a) - V_\pi (s), 
\end{align}
The action-value function $Q$ estimates the discounted total reward following the policy $\pi$ after taking the action $a$ at the current state $s_t$. The value function $V$ estimates the discounted total reward following the policy $\pi$ both at the current state and in the future. The advantage function $A$ is the difference between $Q$ and $V$, which indicates how much better is taking the action $a$ than sampling an action from the current policy $\pi$.
The following recursive definitions hold:
\begin{align}
Q_\pi(s_t,a) &= \mathbb{E}_{s_{t+1}}
\big[ r + \gamma V_\pi(s_{t+1}) \big], \\
V_\pi(s_t) &= \mathbb{E}_{a_{t},s_{t+1}}
\big[ r + \gamma V_\pi(s_{t+1}) \big],\\
& \ \ \ \ \text{where } a_t \sim \pi (a_t|s_t), \text{for } t \geq 0  \\
\end{align}



\section{Scope of Study}
Realistic robot control problems in reinforcement learning involve many sources of complexity. However, we are only concerned with the major ones.
We focus on a set of reinforcement learning problems with multi-modality state space and continuous action space. We also specifically study the problems that not only have the previously mentioned properties, but also have sparse reward functions.

\subsection{Multi-modality State Space}

The state spaces of a vast majority of the previously studied reinforcement learning problems are in single-modality, such as a low-dimensional vector like internal game state and locomotion sensor signal, or an image stream. Most of the recent works in reinforcement learning for continuous control focus on the RL tasks with low-dimensional state input only~\cite{duan2016benchmarking}. Some ~\cite{wu2017scalable} also tried to handle tasks with limited complexity that takes an image-input.

However, it is more common in real-world environments that a robot agent has multiple types of sensors installed simultaneously. In such cases, the state space of the reinforcement learning problem is a combination of input with different modalities. The different modalities in the state space could provide complementary information that cannot be substituted by each other. For a typical robot locomotion problem, the low-dimensional motion sensor input usually provides accurate information about the agent's physical status. The image input, on the other hand, may provide information about the external environments and object. If the motion sensor input was missing, the robot agent would not be able to perform motor control accurately. On the other hand, if the image sensor is missing, the robot agent would not be able to know anything about its task and the environment.


Apart from motion and image sensors, real-world robot agents may have other modalities such as wireless network antenna signals, human language instructions and LIDAR sensors. This thesis focus on robot control environments whose state input has two modalities, including a low-dimensional motion sensor input and an image sensor input. This setting is among the most typical cases of real-world robot problems.

In the context of reinforcement learning, the previously described multi-modality problem is significantly more difficult than single-modality problems. The reason is that the features of image observations are much more difficult to learn than the features of motion sensor signals. End-to-end reinforcement learning methods would typically focus on extracting information from the low-dimensional input in the initial phase of training, and get stuck until the agent is able to extract useful information from the image input. This local-minimum problem is more challenging than the case in supervised learning, because the generation of training samples are also controlled by the agent's policy.

\subsection{Sparse Reward Function}
The sparse/delayed reward signal problem has been one of the major problems in reinforcement learning domains for the past 20 years. In the setting of reinforcement learning problems with sparse rewards, informative reward signals are very rare. For example, the agent may just receive a constant negative reward at most time but only receives a positive reward when some rare event happens. The agent may not be able to learn anything if it has never triggered such an event. The agent may also not be able to learn an optimal policy if it has experienced such an event but it happens too late that the agent has already been very conservative in the degree of exploration. The major problem is to perform efficient exploration.

Many methods have been proposed to solve the sparse reward problem in environments. However, none of the methods are able to solve the general case in continuous control context. Most of them could only solve special cases like finite state and finite action problems, which are far from realistic problems.

A smooth and informative reward function can generally make the training task easier. Theoretically, any reinforcement learning problems when given a reward function that is informative enough could be solved easily by current state-of-art methods. However, such a reward function itself is rarely accessible in real-world environments. Reward shaping~\cite{ng1999policy} techniques may be used to improve the reward function, but such methods usually also rely on domain-specific engineering. In our study, we would like to solve several realistic sparse environments without the application of reward shaping.


\subsection{Assumptions on Relationship of Tasks}
Apart from the objective to solve sparse multi-modal reinforcement learning problems with end-to-end flat reinforcement learning methods, we are also particularly interested in a paradigm, where the objective is to solve a \textit{target task} given a set of \textit{source tasks}. The policies that solve the source tasks, namely the \textit{actuator policies}, are available to the agent when it is faced with the target task. We call them actuator policies because these policies perform low-level control skills instead of making the highest level decision. We assume that the target task can be solved by a sequential execution of a subset of the actuator policies. A \textit{decider agent} needs to be trained, which controls the selection and scheduling of the actuator policies.

This paradigm is related to hierarchical reinforcement learning (HRL), which focuses on learning and utilizing closed-loop partial-policy/skills to solve a complex task. However, our case is different from the standard setting of HRL. The standard HRL setting assumes that the agent needs to extract the useful closed-loop partial policy from the target task. However, we focus on extracting and reusing partial policies from other tasks.

This paradigm is also related to transfer learning in reinforcement learning~\cite{taylor2009transfer}, because the source-task policies are reused by the decider agent to solve another task. However, conventional transfer learning paradigms don't pose assumptions on a hierarchical relationship between the target task and the source tasks.

This is the only assumption on the relationship between the source tasks and the target task. It is not known whether any specific source task is useful. A source task policy might be useful in some cases, or it could be completely useless. The execution time of an actuator policy is also unknown. An actuator policy might be used for only 1 time-step before the agent switch to another one, or it might also be used for the whole episode.

Since the set of source-task policies is pre-defined and there is no autonomous method for selecting then, the selection of the source tasks is considered a domain-specific design component. However, it is common that real-world complex problems can be decomposed into primary tasks. For example, classical robot locomotion methods organize the abstract problems (such as trajectory planning) and low-level problems (such as motor control) in hierarchical structures. Apart from that, it is also common that human beings would decompose the original problem into a series of easier problems when the problem it faced is too complex.

If this hierarchical task relationship assumption was not present, the problem would belong to the class of the fully autonomous hierarchical reinforcement learning problem. In this case, the agents need to learn not only a task structure but also all actuator policies from scratch. The problem is currently considered in-feasible to solve for general cases~\cite{barto2003recent}.


\section{Research Questions}


We will propose several reinforcement learning environments for continuous control in section \ref{sec_env} according to the stated scope, and the proposed problems will be of the main focus of the study.
The primary research question that we would like to study is:

    \textit{Can we develop an agent that can achieve sufficient performance (in terms of total episode reward) in the reinforcement learning environments defined in section \ref{sec_env} within a feasible training time, with an RL model that doesn't rely on domain-specific techniques  (including reward shaping, manual definition of policy hierarchy, heuristic methods to define sub-goals, or manual criteria on the switching of actuator policies), with or without a predefined set of source tasks?}


We will first answer the question on whether a hierarchical architecture is necessary: 

\textbf{Research Question 1: }\textit{Can a contemporary flat reinforcement learning method solve the proposed environments?}


We will then answer the question on whether an end-to-end hierarchical reinforcement learning can solve this:
A model that solves the question needs to address two issues: reusing the policies learned in source tasks and solving the target task. The two issues lead to the following two research questions respectively:

\textbf{Research Question 2.1: }\textit{Can a trained actuator policy achieve a good performance (in terms of the expected total return of the original source task) in a target environment, which might have different initial state distribution?}.

\textbf{Research Question 2.1: }\textit{Can a decider agent be developed to solve the target tasks defined in \ref{sec_env} given the trained actuators policies, without other domain-specific components?}.
