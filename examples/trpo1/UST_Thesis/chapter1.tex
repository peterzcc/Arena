%!TEX program = xelatex
%!TEX root = ./thesis.tex

\chapter{Introduction}
\section{Overview}
Deep reinforcement learning methods have become popular in the machine learning field in the recent years. Through the combination of deep neural network models and reinforcement learning theory, advances in deep reinforcement learning have achieved human level control performance in playing Atari games \cite{mnih2015human}, mastered the board game GO \cite{silver2016mastering}, learned to play Texas Hold'em pokers \cite{heinrich2016deep}, to solve simple robot control tasks \cite{duan2016benchmarking} and to play a simplified version of the game Dota 2~\cite{openai_2018}.

However, the capability of contemporary deep reinforcement learning methods is still limited when we consider real-world problems, such as playing the StarCraft games, controlling robots to do meaningful tasks and making stock trading decisions. Generally speaking, the contemporary end-to-end deep reinforcement learning methods have been able to solve two classes of problems. The first class of problems consists of single-agent environments with unknown state-transition dynamics but trivial task logics, such as the Atari games. The second class of problems consists of the multi-agent games which could have relatively complex task logics but in a closed system, such as the board game Go. 

General real-world robot problems, however, are more challenging. Compared with video game environments like~\cite{mnih2015human}, robot problems have continuous action spaces instead of discrete spaces. This has made the policy exploration problem much more difficult. Apart from that, most robot control problems have multi-modal state space consisting both low-dimensional and high-dimensional inputs. The multi-modality property further introduces complexity to reinforcement learning problems because the local-minimum problem is much more difficult than in supervised learning domains. Finally, the lack of smooth and informative reward signals in realistic robot problems pose significant challenges to contemporary end-to-end deep reinforcement learning methods.

In this study, we investigate a set of challenging robot control environments and propose flat and hierarchical reinforcement learning methods to solve them.
%TODO elaborate
\section{Background in Reinforcement Learning (RL)}
Reinforcement learning~\cite{sutton1998reinforcement} is the problem of learning a policy that map from situations to actions in order to maximize a numerical reward signal. The learner must discover the optimal policy by examining different possibilities. Reinforcement learning is different from supervised learning that the learner is not provided labeled examples by a knowledgeable external supervisor. Apart from that, reinforcement learning agents are trained on the data that are experiences generated by itself, instead of any existing training data. Therefore, reinforcement learning agents not only need to exploit their experience, but also need to generate experience that efficiently explore the problem space.  Reinforcement learning is also different from unsupervised learning, whose objective is finding hidden structures in unlabeled data without any guiding signal.

Most reinforcement learning study is based on the problem formulation of Markov Decision Processes (MDPs). MDP is a framework defining the problem of learning from interaction. The learner is called the \textit{agent}. Everything outside that interacts with the agent is called the \textit{environment}.
The agent and environments interact during a finite sequence of discrete time steps, $t=0,1,2,3,...$. At each time step t, the agent receives an observation of the environment's \textit{state}, $s \in S$ and selects an \textit{action} $a \in A$. The agent receives a numerical reward $r$, and also the next state $s^\prime$.  A finite-horizon discounted Markov decision process (MDP) is defined by the tuple $(S,A,P,r,\rho_0,\gamma) $ where the $\rho_0 : S \mapsto \mathbb{R}$ is the initial state distribution, $P(s^\prime|s,a) : S \times A \mapsto \mathbb{R}$ is the distribution describing the state transition dynamics, $r : S \times A \mapsto \mathbb{R}$ is the expected reward function, and $\gamma \in (0,1]$ is the discount factor.

Let $\pi : S \times A \mapsto [0,1] $ denote a policy, following are definitions of the action value function $Q_\pi $, the value function $V_\pi $ and the advantage function $A_\pi $:
$$ Q_\pi(s_t,a_t) = \mathbb{E}_{s_{t+1},a_{t+1},\ldots}
\big[ \sum_{l=0}^\infty r(s_{t}) \big],$$
$$ V_\pi(s_t) = \mathbb{E}_{a_{t},s_{t+1},a_{t+1},\ldots}
\big[ \sum_{l=0}^\infty r(s_{t}) \big],$$

$$ A_\pi (s,a) = Q_\pi (s,a) - V_\pi (s), \text{where } a_t \sim \pi (a_t|s_t), \text{for } t \geq 0 $$

%%TODO: think about where I can put this section
%\subsection{Hierarchical reinforcement learning based on transferred policies from a set of source tasks}\label{sec_hier_trans}



\section{Scope of study}
Realistic robot control problems in reinforcement learning involves many sources of complexity. However, we are only concerned with some of the typical problems.
This study focus on a set of reinforcement learning problems with multi-modality state space and continuous action space. We also specifically study some problems with not only the previously mentioned properties, but also with sparse reward functions.

\subsection{Multi-modality state space}

The state spaces of a vast majority of the problems in previous reinforcement learning studies are in single-modality, such as a low-dimensional vector of internal game state or locomotion sensor signal, or a single high dimensional image stream. Most recent works in reinforcement learning for continuous control focus on the RL tasks with low-dimensional state input only~\cite{duan2016benchmarking}. Some study~\cite{wu2017scalable} also tried to handle tasks with a single image-input but in limited complexity.

However, it is more common in real-world environments that a robot agent has multiple types of sensors installed simultaneously. In such case, the state space of the reinforcement learning problem is a combination of input with different modalities. The different modalities in the state space could provide complementary information that cannot be substituted. For a typical robot locomotion problem, the low-dimensional motion sensor input usually provides accurate information about the agent's physical status. The image input, on the other hand, may provide information on the external environments and object relationships. If the motion sensor input is missing, the robot agent would not perform its motor control well, on the other hand, if the image sensor is missing, the robot agent would not know anything about its task and environment.


Apart from motion sensor and image sensor, real-world robot agents may have other modalities such as wireless network antenna input, human language instructions and laser sensor. This thesis focus on robot control environments whose state observation consists of a low-dimensional motion sensor signal and an image sensor stream, which is among the most typical cases of real-world robot problems.

In the context of reinforcement learning, the previously described multi-modality problem is significantly more complex than single-modality problems. The reason is that the feature of image observations is much more difficult to learn than the motion sensor signals. End-to-end reinforcement learning methods would typically prioritize the low-dimensional input in the initial phase of training, and get stuck until the agent is able to extract useful information from the image input.

\subsection{Sparse reward function}
The problem of sparse / delayed reward signal has been one of the major problems to be tackled in reinforcement learning. Many methods are proposed to solve the sparse reward problem in environments with discrete action spaces. However, the problem is much more difficult for the environments with continuous action spaces, and few works have focused on the class of system. By the time of writing, we haven't seen any method that has been proposed to effectively solve continuous control tasks with sparse rewards.

Studies have shown that smooth and informative reward function can generally make the training of reinforcement learning agents easier. Theoretically, any reinforcement learning problems when given a reward function that is good enough.

However, the reward function of a realistic robot control environment is usually discontinuous and sparse. Reward shaping techniques may improve the reward function, but it usually relies on domain-specific engineering. In our study, we try to solve some realistic robot problems without reward shaping, which essentially indicates the problems have sparse reward functions.


\subsection{Assumptions on relationship of tasks}

We are particularly interested in a class of reinforcement learning problems, where the objective is to solve a \textit{target task } given a set of trained \textit{source tasks}. The policies that can solve the source tasks, namely the \textit{actuator policies}, are available for the agent. We assume that the target task can be solved by a sequential execution of a subset of the source-task policies. An \textit{decider agent} needs to be trained, which chooses the source-task policy to be executed and decides when to switch between them.

This paradigm is related to hierarchical reinforcement learning (HRL), which focuses on learning and utilizing closed-loop partial policy / skills to solve a complex task. However, our case is different from the standard setting of HRL where the agent needs to extract the useful closed-loop partial policy from the original task. However, we focus on extracting and reusing partial policies from other tasks.

This paradigm is also related to transfer learning in reinforcement learning~\cite{taylor2009transfer}, because the source-task policies are reused by the high-level scheduler agent. However, conventional transfer learning paradigms usually don't assume a hierarchical relationship between the target task and the source tasks.

This is the only assumption on the relationship between the source tasks and the target task. It is not known whether any specific source task is useful. A source task policy might be useful in some cases, or it could be completely useless. The minimum execution length of an actuator policy is also unknown. An actuator policy might be executed for only 1 time-step before the agent switch to another one, or it could be executed for the whole episode.

This assumption needs domain-specific design on the selection of source tasks. However, it is common that real-world complex robot control problems can be decomposed to primary tasks. For example, classical robot locomotion methods organize the abstract problems (such as trajectory planning) and low level problems (such as motor control) in hierarchical structures.

If this hierarchical task relationship assumption was removed, the problem becomes the fully autonomous hierarchical reinforcement learning problem, where the agents need to learn not only a task structure but also all actuator policies from scratch. This problem is currently considered in-feasible to solve for general robot control problems~\cite{barto2003recent}.


\section{Research Questions}
% TODO: there should be a larger set of task settings, to prove that the method is effective for general tasks.
% an have the same set of source tasks with different target tasks
% The questions should be specific that the experiment results and comparison can show the answer clearly, with measurable quantity
% Specific performance metric
% just define the proposed tasks and reference it to the later section, state that the tasks are more realistic


We will propose several reinforcement learning environments for continuous control in section \ref{sec_env} according to the stated scope and assumption. The study will mainly focus on solving the proposed problems.
The primary research question that we focus on is:
\begin{center}
    \textit{Can we develop an agent that can achieve good performance (in terms of total reward) in the reinforcement learning environments defined in section \ref{sec_env} within reasonable training time, with an RL model that doesn't rely on domain-specific techniques  (including reward shaping, manual definition of policy hierarchy, manual / heuristic methods to define sub-goals, or manual criteria on the switching of actuator policies) given a predefined set of source tasks?}
\end{center}

We will first answer the question on whether the previous work already solve the problem: \textit{Can the contemporary flat reinforcement learning methods solve the proposed environments?}


We will then answer the question on whether an end-to-end hierarchical reinforcement learning can solve this:
A model that solves the primary question needs to address two issues: reusing the policies learned in source tasks and solving the target task.

To solve the first issue, we need to answer the question of "\textit{Can a trained actuator policy achieve an acceptable performance (in terms of the expected total return of the original source task), with or without fine-tuning methods, in a target environment, which usually has different initial state distribution?}".

To address the second issue, we will answer the question of "\textit{Can a hierarchical reinforcement learning agent be developed to solve the target tasks defined in \ref{sec_env} given the trained actuators policies, without other domain-specific designs?}".

% \section{Thesis Outline and Contributions}
% The remainder of this thesis is structured as follows. TODO