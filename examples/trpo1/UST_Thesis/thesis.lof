\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The hierarchical structure of the modulated controller\nobreakspace {}\cite {heess2016learning}, which consists of the recurrent high-level controller (HL, dark blue) and feedforward low-level controller (LL, green). The high-level controller has access to all observations (yellow and red). While both controllers observe sensory input at the world-clock frequency, the modulatory control signal from the high-level is updated every K steps (here K = 2)\relax }}{15}{figure.caption.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces The agent setup of the modulated controller\nobreakspace {}\cite {frans2017meta},$\theta $ represents the master policy, which selects a sub-policy to be active. In the diagram, $\phi _3$ is the active sub-policy, and actions are taken according to its output.\relax }}{16}{figure.caption.2}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces The Ant agent\relax }}{20}{figure.caption.3}
\contentsline {figure}{\numberline {3.2}{\ignorespaces The Humanoid agent\relax }}{20}{figure.caption.4}
\contentsline {figure}{\numberline {3.3}{\ignorespaces The Swimmer agent\relax }}{20}{figure.caption.5}
\contentsline {figure}{\numberline {3.4}{\ignorespaces A sample image observation of the target environments\relax }}{21}{figure.caption.6}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Inconsistent performance produced by ACKTR agents with the same parameters but different runs. The vertical axis is the total return averaged over the recent 20 episodes and the horizontal axis is the number of million time-steps\relax }}{33}{figure.caption.8}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Performance of ACKTR agents with different KL-divergence constraints. All the agents are trained with batch-size 4000 and 20 parallel agents. The vertical axis is the total return averaged over the recent 200 episodes and the horizontal axis is the number of million time-steps\relax }}{33}{figure.caption.9}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Performance of W-KTR agents with different W2-metric constraints. All the agents are trained with batch-size 4000 and 20 parallel agents. The vertical axis is the total return averaged over the recent 20 episodes and the horizontal axis is the number of million time-steps\relax }}{34}{figure.caption.10}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Performance of a W-KTR agent with a decaying W2-metric from 0.02 to 0.00003 in the first 15 million time-steps. The agent is trained with batch-size 4000 and 20 parallel agents. The vertical axis is the total return averaged over the recent 20 episodes and the horizontal axis is the number of million time-steps\relax }}{35}{figure.caption.11}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Performance of agents with different weight on entropy regularization term, the horizontal axis is the number of million timesteps and the vertical axis is the total episode reward averaged over the last 32 episodes\relax }}{37}{figure.caption.12}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Average standard deviation of the policy of agents in Figure\nobreakspace {}\ref {rec_ent_reg}, the horizontal axis is the number of million timesteps.\relax }}{38}{figure.caption.13}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Performance of ACKTR agent on the moveg2 task, the horizontal axis is the number of million timesteps and the vertical axis is the total episode reward averaged over the last 20 episodes\relax }}{38}{figure.caption.14}
\contentsline {figure}{\numberline {4.8}{\ignorespaces The avearge reward per time-step of ACKTR agent on the moveg2 task, the horizontal axis is the number of training batches\relax }}{39}{figure.caption.15}
\contentsline {figure}{\numberline {4.9}{\ignorespaces The avearge standard deviation of ACKTR agent's policy on the moveg2 task, the horizontal axis is the number of training batches\relax }}{39}{figure.caption.16}
\contentsline {figure}{\numberline {4.10}{\ignorespaces The distribution of advantage values of the ACKTR agent at epoch 0, 3000 and 4900 respectively. The horizontal axis is the log-likelihood value\relax }}{41}{figure.caption.17}
\contentsline {figure}{\numberline {4.11}{\ignorespaces Performance of agents with different exceptional advantage regularization weights, the horizontal axis is the number of million time-steps and the vertical axis is the total episode reward averaged over the last 32 episodes\relax }}{42}{figure.caption.18}
\contentsline {figure}{\numberline {4.12}{\ignorespaces The avearge standard deviation of agents with different exceptional advantage regularization weights, the horizontal axis is the number of million time-steps and the vertical axis is the total episode reward averaged over the last 32 episodes\relax }}{43}{figure.caption.19}
\contentsline {figure}{\numberline {4.13}{\ignorespaces Performance of ACKTR agents with different KL-divergence constraints, the $x$-axis is the number of million time-steps and the $y$-axis is the total episode reward averaged over the last 32 episodes.\relax }}{44}{figure.caption.20}
\contentsline {figure}{\numberline {4.14}{\ignorespaces Performance of actuator agents with domain randomization by cross-sampling initial states, the horizontal axis is the number of million time-steps and the vertical axis is the total episode reward averaged over the last 200 episodes\relax }}{46}{figure.caption.21}
\contentsline {figure}{\numberline {4.15}{\ignorespaces Performance of actuator agents using synchronous scheduling of actuator learning , the horizontal axis is the number of million time-steps and the vertical axis is the total episode reward averaged over the last 200 episodes\relax }}{47}{figure.caption.22}
\contentsline {figure}{\numberline {4.16}{\ignorespaces Decision policy training performance of the task dynamicg8, the horizontal axis is the number of million time-steps and the vertical axis is the total episode reward averaged over the last 32 episodes\relax }}{49}{figure.caption.24}
\contentsline {figure}{\numberline {4.17}{\ignorespaces Decision policy training performance of the task reachcont, the horizontal axis is the number of million time-steps and the vertical axis is the total episode reward averaged over the last 200 episodes\relax }}{50}{figure.caption.25}
\contentsline {figure}{\numberline {4.18}{\ignorespaces Switcher policy training performance of the task dynamicg8, the horizontal axis is the number of million time-steps and the vertical axis is the total episode reward averaged over the last 200 episodes\relax }}{51}{figure.caption.27}
\contentsline {figure}{\numberline {4.19}{\ignorespaces Average execution length of the actions of decision policies of the task dynamicg8, the horizontal axis is the number of million time-steps and the vertical axis is the decision policy's average execution length of the last batch.\relax }}{52}{figure.caption.28}
\contentsline {figure}{\numberline {4.20}{\ignorespaces Switcher policy training performance of the task reachcont, the horizontal axis is the number of million time-steps and the vertical axis is the total episode reward averaged over the last 32 episodes\relax }}{53}{figure.caption.29}
\contentsline {figure}{\numberline {4.21}{\ignorespaces Average execution length of the actions of decision policies of the task reachcont, the horizontal axis is the number of million time-steps and the vertical axis is the decision policy's average execution length of the last batch.\relax }}{54}{figure.caption.30}
\addvspace {10\p@ }
