\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The hierarchical structure of the modulated controller\nobreakspace {}\cite {heess2016learning}, which consists of the recurrent high-level controller (HL, dark blue) and feedforward low-level controller (LL, green). The high-level controller has access to all observations (yellow and red). While both controllers observe sensory input at the world-clock frequency, the modulatory con- trol signal from the high-level is updated every K steps (here K = 2)}}{13}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces The agent setup of the modulated controller\nobreakspace {}\cite {frans2017meta},$\theta $ represents the master policy, which selects a sub-policy to be active. In the diagram, $\phi _3$ is the active sub-policy, and actions are taken according to its output.}}{14}{figure.2.2}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces The "Ant" agent}}{18}{figure.3.1}
\contentsline {figure}{\numberline {3.2}{\ignorespaces The "Humanoid" agent}}{18}{figure.3.2}
\contentsline {figure}{\numberline {3.3}{\ignorespaces The "Swimmer" agent}}{18}{figure.3.3}
\contentsline {figure}{\numberline {3.4}{\ignorespaces A sample image observation of the target environments}}{19}{figure.3.4}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Inconsistent performance produced by ACKTR agents with the same parameters but different runs. The vertical axis is the total return averaged over the recent 20 episodes and the horizontal axis is the number of million time-steps}}{29}{figure.4.1}
\contentsline {figure}{\numberline {4.2}{\ignorespaces Performance of ACKTR agents with different KL-divergence constraints. All the agents are trained with batch-size 4000 and 20 parallel agents. The vertical axis is the total return averaged over the recent 200 episodes and the horizontal axis is the number of million time-steps}}{29}{figure.4.2}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Performance of W-KTR agents with different W2-metric constraints. All the agents are trained with batch-size 4000 and 20 parallel agents. The vertical axis is the total return averaged over the recent 200 episodes and the horizontal axis is the number of million time-steps}}{30}{figure.4.3}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Performance of a W-KTR agent with a decaying W2-metric from 0.02 to 0.00003 in the first 15 million time-steps. The agent is trained with batch-size 4000 and 20 parallel agents. The vertical axis is the total return averaged over the recent 200 episodes and the horizontal axis is the number of million time-steps}}{31}{figure.4.4}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Performance of agents with different weight on entropy regularization term, the horizontal axis is the number of million timesteps and the vertical axis is the total episode reward averaged over the last 32 episodes}}{33}{figure.4.5}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Logarithm of average standard deviation of the policy of agents in Figure\nobreakspace {}\ref {rec_ent_reg}, the horizontal axis is the number of million timesteps.}}{34}{figure.4.6}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Performance of ACKTR agent on the "moveg2" task, the horizontal axis is the number of million timesteps and the vertical axis is the total episode reward averaged over the last 20 episodes}}{35}{figure.4.7}
\contentsline {figure}{\numberline {4.8}{\ignorespaces The avearge reward per time-step of ACKTR agent on the "moveg2" task, the horizontal axis is the number of training batches}}{35}{figure.4.8}
\contentsline {figure}{\numberline {4.9}{\ignorespaces The logarithm of avearge standard deviation of ACKTR agent's policy on the "moveg2" task, the horizontal axis is the number of training batches}}{36}{figure.4.9}
\contentsline {figure}{\numberline {4.10}{\ignorespaces The distribution of advantage values of the ACKTR agent at batch 0 on the "moveg2" task, the horizontal axis is the log-likelihood value}}{37}{figure.4.10}
\contentsline {figure}{\numberline {4.11}{\ignorespaces The distribution of advantage values of the ACKTR agent at batch 300 on the "moveg2" task, the horizontal axis is the log-likelihood value}}{38}{figure.4.11}
\contentsline {figure}{\numberline {4.12}{\ignorespaces The distribution of advantage values of the ACKTR agent at batch 4900 on the "moveg2" task, the horizontal axis is the log-likelihood value}}{39}{figure.4.12}
\contentsline {figure}{\numberline {4.13}{\ignorespaces Performance of agents with different exceptional advantage regularization weights, the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 32 episodes}}{40}{figure.4.13}
\contentsline {figure}{\numberline {4.14}{\ignorespaces The logarithm of avearge standard deviation of agents with different exceptional advantage regularization weights, the horizontal axis is the number of million time-steps and the vertical axis is the total episode reward averaged over the last 32 episodes}}{41}{figure.4.14}
\contentsline {figure}{\numberline {4.15}{\ignorespaces Performance of ACKTR agents with different KL-divergence constraints, the x-axis is the number of million time-steps and the y-axis is the total episode reward averaged over the last 32 episodes}}{42}{figure.4.15}
\addvspace {10\p@ }
