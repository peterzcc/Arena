\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces The hierarchical structure of the modulated controller\nobreakspace {}\cite {heess2016learning}, which consists of the recurrent high-level controller (HL, dark blue) and feedforward low-level controller (LL, green). The high-level controller has access to all observations (yellow and red). While both controllers observe sensory input at the world-clock frequency, the modulatory con- trol signal from the high-level is updated every K steps (here K = 2)}}{13}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces The agent setup of the modulated controller\nobreakspace {}\cite {frans2017meta},$\theta $ represents the master policy, which selects a sub-policy to be active. In the diagram, $\phi _3$ is the active sub-policy, and actions are taken according to its output.}}{14}{figure.2.2}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces The "Ant" agent}}{18}{figure.3.1}
\contentsline {figure}{\numberline {3.2}{\ignorespaces The "Humanoid" agent}}{18}{figure.3.2}
\contentsline {figure}{\numberline {3.3}{\ignorespaces The "Swimmer" agent}}{18}{figure.3.3}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Performance of agents with different weight on exceptional advantage regularization, the x-axis is the number of million timesteps and the y-axis is the total episode reward averaged over the last 32 episodes}}{28}{figure.4.1}
\addvspace {10\p@ }
\addvspace {10\p@ }
