\contentsline {chapter}{Title Page}{i}{Doc-Start}
\contentsline {chapter}{Authorization Page}{ii}{Doc-Start}
\contentsline {chapter}{Signature Page}{iii}{Doc-Start}
\contentsline {chapter}{Acknowledgments}{iv}{Doc-Start}
\contentsline {chapter}{Table of Contents}{v}{Doc-Start}
\contentsline {chapter}{List of Figures}{viii}{Doc-Start}
\contentsline {chapter}{List of Tables}{xi}{Doc-Start}
\contentsline {chapter}{Abstract}{xii}{Doc-Start}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Overview}{1}{section.1.1}
\contentsline {section}{\numberline {1.2}Background in Reinforcement Learning (RL)}{2}{section.1.2}
\contentsline {section}{\numberline {1.3}Scope of Study}{3}{section.1.3}
\contentsline {subsection}{\numberline {1.3.1}Multi-modality State Space}{3}{subsection.1.3.1}
\contentsline {subsection}{\numberline {1.3.2}Sparse Reward Function}{4}{subsection.1.3.2}
\contentsline {subsection}{\numberline {1.3.3}Assumptions on Relationship of Tasks}{5}{subsection.1.3.3}
\contentsline {section}{\numberline {1.4}Research Questions}{6}{section.1.4}
\contentsline {chapter}{\numberline {2}Related Works}{8}{chapter.2}
\contentsline {section}{\numberline {2.1}Policy Gradient Methods}{8}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}Trust Region Policy Optimization}{9}{subsection.2.1.1}
\contentsline {subsection}{\numberline {2.1.2}Kronecker-factored Trust Region Policy Gradient}{10}{subsection.2.1.2}
\contentsline {subsection}{\numberline {2.1.3}Proximal Policy Gradient Method}{11}{subsection.2.1.3}
\contentsline {subsection}{\numberline {2.1.4}Other Methods in Deep Reinforcement Learning for Continuous Control}{11}{subsection.2.1.4}
\contentsline {section}{\numberline {2.2}Hierarchical Reinforcement Learning Methods}{12}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Option Framework}{13}{subsection.2.2.1}
\contentsline {subsection}{\numberline {2.2.2}Modulated Hierarchical Controller}{14}{subsection.2.2.2}
\contentsline {subsection}{\numberline {2.2.3}Hierarchical Reinforcement Learning by Meta-learning}{16}{subsection.2.2.3}
\contentsline {subsection}{\numberline {2.2.4}Goal-directed Learning Methods}{17}{subsection.2.2.4}
\contentsline {subsection}{\numberline {2.2.5}Other Methods Targeting at Environments with Sparse Reward Function}{18}{subsection.2.2.5}
\contentsline {chapter}{\numberline {3}Methodology}{19}{chapter.3}
\contentsline {section}{\numberline {3.1}Target Environments}{19}{section.3.1}
\contentsline {subsection}{\numberline {3.1.1}Summary of Environment Design}{19}{subsection.3.1.1}
\contentsline {subsection}{\numberline {3.1.2}Detailed Environment Specifications}{21}{subsection.3.1.2}
\contentsline {section}{\numberline {3.2}Wasserstein Actor Critic Kronecker-factored Trust Region Policy Optimization Method }{22}{section.3.2}
\contentsline {section}{\numberline {3.3}Efficient Exploration Through Exceptional Advantage Regularization}{25}{section.3.3}
\contentsline {section}{\numberline {3.4}Efficient Exploration Through Robust Concentric Gaussian Mixture Policy}{26}{section.3.4}
\contentsline {section}{\numberline {3.5}Hierarchical Reinforcement Learning method}{27}{section.3.5}
\contentsline {subsection}{\numberline {3.5.1}Hierarchical Reinforcement Learning Architecture}{27}{subsection.3.5.1}
\contentsline {subsection}{\numberline {3.5.2}Generalized advantage estimation for the decider agent}{28}{subsection.3.5.2}
\contentsline {subsection}{\numberline {3.5.3}Training of the Switcher Policy}{29}{subsection.3.5.3}
\contentsline {section}{\numberline {3.6}Training of the Actuator Policies}{30}{section.3.6}
\contentsline {subsection}{\numberline {3.6.1}Effective Training of Actuator Policies}{30}{subsection.3.6.1}
\contentsline {subsection}{\numberline {3.6.2}Domain Randomization by Cross-sampling Initial States}{31}{subsection.3.6.2}
\contentsline {subsection}{\numberline {3.6.3}Synchronous Scheduling of Actuator Learning}{31}{subsection.3.6.3}
\contentsline {chapter}{\numberline {4}Experiments}{32}{chapter.4}
\contentsline {section}{\numberline {4.1}Experiments on the Basic Task \textit {move0}}{32}{section.4.1}
\contentsline {section}{\numberline {4.2}Experiment on Flat Reinforcement Learning Solutions to Multi-modality Tasks}{35}{section.4.2}
\contentsline {subsection}{\numberline {4.2.1}Conventional Flat reinforcement Learning Methods}{35}{subsection.4.2.1}
\contentsline {subsection}{\numberline {4.2.2}Experiment on Exceptional Advantage Regularization}{36}{subsection.4.2.2}
\contentsline {subsection}{\numberline {4.2.3}Experiment on Robust Concentric Gaussian Mixture Policy Model}{40}{subsection.4.2.3}
\contentsline {section}{\numberline {4.3}Experiments on Hierarchical Reinforcement Learning Methods for Multi-modality and Sparse environments}{45}{section.4.3}
\contentsline {subsection}{\numberline {4.3.1}Training the Actuator Agents with Domain Randomization by Cross-sampling Initial States}{45}{subsection.4.3.1}
\contentsline {subsection}{\numberline {4.3.2}Training the Decider Agent}{47}{subsection.4.3.2}
\contentsline {subsubsection}{Phase 1: Decision policy training}{48}{section*.23}
\contentsline {subsubsection}{Phase 2: Switcher policy training}{48}{section*.26}
\contentsline {section}{\numberline {4.4}Conclusion on Experiment Results}{55}{section.4.4}
\contentsline {chapter}{\numberline {5}Discussion}{56}{chapter.5}
\contentsline {chapter}{Reference}{58}{chapter.5}
