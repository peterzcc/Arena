\contentsline {chapter}{Title Page}{i}{Doc-Start}
\contentsline {chapter}{Authorization Page}{ii}{Doc-Start}
\contentsline {chapter}{Signature Page}{iii}{Doc-Start}
\contentsline {chapter}{Acknowledgments}{iv}{Doc-Start}
\contentsline {chapter}{Table of Contents}{v}{Doc-Start}
\contentsline {chapter}{List of Figures}{viii}{Doc-Start}
\contentsline {chapter}{List of Tables}{xi}{Doc-Start}
\contentsline {chapter}{Abstract}{xii}{Doc-Start}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Overview}{1}{section.1.1}
\contentsline {section}{\numberline {1.2}Background in Reinforcement Learning (RL)}{2}{section.1.2}
\contentsline {section}{\numberline {1.3}Scope of Study}{3}{section.1.3}
\contentsline {subsection}{\numberline {1.3.1}Multi-modality State Space}{3}{subsection.1.3.1}
\contentsline {subsection}{\numberline {1.3.2}Sparse Reward Function}{4}{subsection.1.3.2}
\contentsline {subsection}{\numberline {1.3.3}Assumptions on Relationship of Tasks}{5}{subsection.1.3.3}
\contentsline {section}{\numberline {1.4}Research Questions}{6}{section.1.4}
\contentsline {chapter}{\numberline {2}Related Works}{8}{chapter.2}
\contentsline {section}{\numberline {2.1}Policy Gradient Methods}{8}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}Trust Region Policy Optimization}{9}{subsection.2.1.1}
\contentsline {subsection}{\numberline {2.1.2}Kronecker-factored Trust Region Policy Gradient}{9}{subsection.2.1.2}
\contentsline {subsection}{\numberline {2.1.3}Proximal Policy Gradient Method}{10}{subsection.2.1.3}
\contentsline {subsection}{\numberline {2.1.4}Other Methods in Deep Reinforcement Learning for Continuous Control}{11}{subsection.2.1.4}
\contentsline {section}{\numberline {2.2}Hierarchical Reinforcement Learning Methods}{11}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Option Framework}{12}{subsection.2.2.1}
\contentsline {subsection}{\numberline {2.2.2}Modulated Hierarchical Controller}{13}{subsection.2.2.2}
\contentsline {subsection}{\numberline {2.2.3}Hierarchical Reinforcement Learning by Meta-learning}{15}{subsection.2.2.3}
\contentsline {subsection}{\numberline {2.2.4}Goal-directed Learning Methods}{16}{subsection.2.2.4}
\contentsline {subsection}{\numberline {2.2.5}Other Methods Targeting at Environments with Sparse Reward Function}{17}{subsection.2.2.5}
\contentsline {chapter}{\numberline {3}Methodology}{18}{chapter.3}
\contentsline {section}{\numberline {3.1}Target Environments}{18}{section.3.1}
\contentsline {subsection}{\numberline {3.1.1}Summary of Environment Design}{18}{subsection.3.1.1}
\contentsline {subsection}{\numberline {3.1.2}Detailed Environment Specifications}{20}{subsection.3.1.2}
\contentsline {section}{\numberline {3.2}Wasserstein Actor Critic Kronecker-factored Trust Region Policy Optimization Method }{21}{section.3.2}
\contentsline {section}{\numberline {3.3}Efficient Exploration Through Exceptional Advantage Regularization}{24}{section.3.3}
\contentsline {section}{\numberline {3.4}Efficient Exploration Through Robust Concentric Gaussian Mixture Policy}{25}{section.3.4}
\contentsline {section}{\numberline {3.5}Hierarchical Reinforcement Learning method}{26}{section.3.5}
\contentsline {subsection}{\numberline {3.5.1}Hierarchical Reinforcement Learning Architecture}{26}{subsection.3.5.1}
\contentsline {subsection}{\numberline {3.5.2}Generalized advantage estimation for the decider agent}{27}{subsection.3.5.2}
\contentsline {subsection}{\numberline {3.5.3}Training of the Switcher Policy}{28}{subsection.3.5.3}
\contentsline {section}{\numberline {3.6}Training of the Actuator Policies}{29}{section.3.6}
\contentsline {subsection}{\numberline {3.6.1}Effective Training of Actuator Policies}{29}{subsection.3.6.1}
\contentsline {subsection}{\numberline {3.6.2}Domain Randomization by Cross-sampling Initial States}{29}{subsection.3.6.2}
\contentsline {subsection}{\numberline {3.6.3}Synchronous Scheduling of Actuator Learning}{30}{subsection.3.6.3}
\contentsline {chapter}{\numberline {4}Experiments}{31}{chapter.4}
\contentsline {section}{\numberline {4.1}Experiments on the Basic Task \textit {move0}}{31}{section.4.1}
\contentsline {section}{\numberline {4.2}Experiment on Flat Reinforcement Learning Solutions to Multi-modality Tasks}{34}{section.4.2}
\contentsline {subsection}{\numberline {4.2.1}Conventional Flat reinforcement Learning Methods}{34}{subsection.4.2.1}
\contentsline {subsection}{\numberline {4.2.2}Experiment on Exceptional Advantage Regularization}{35}{subsection.4.2.2}
\contentsline {subsection}{\numberline {4.2.3}Experiment on Robust Concentric Gaussian Mixture Policy Model}{45}{subsection.4.2.3}
\contentsline {section}{\numberline {4.3}Hierarchical Reinforcement Learning Methods for Multi-modality and Sparse environments}{46}{section.4.3}
\contentsline {subsection}{\numberline {4.3.1}Training the Actuator Agents with Domain Randomization by Cross-sampling Initial States}{46}{subsection.4.3.1}
\contentsline {subsection}{\numberline {4.3.2}Training the Decider Agent}{48}{subsection.4.3.2}
\contentsline {subsubsection}{Phase 1: Decision policy training}{49}{section*.1}
\contentsline {subsubsection}{Phase 2: Switcher policy training}{49}{section*.2}
\contentsline {chapter}{\numberline {5}Discussion}{57}{chapter.5}
\contentsline {chapter}{Reference}{59}{chapter.5}
