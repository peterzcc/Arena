\contentsline {chapter}{Title Page}{i}{Doc-Start}
\contentsline {chapter}{Authorization Page}{ii}{Doc-Start}
\contentsline {chapter}{Signature Page}{iii}{Doc-Start}
\contentsline {chapter}{Acknowledgments}{iv}{Doc-Start}
\contentsline {chapter}{Table of Contents}{v}{Doc-Start}
\contentsline {chapter}{List of Figures}{viii}{Doc-Start}
\contentsline {chapter}{List of Tables}{xi}{Doc-Start}
\contentsline {chapter}{Abstract}{xii}{Doc-Start}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Overview}{1}{section.1.1}
\contentsline {section}{\numberline {1.2}Background in Reinforcement Learning (RL)}{2}{section.1.2}
\contentsline {section}{\numberline {1.3}Scope of study}{3}{section.1.3}
\contentsline {subsection}{\numberline {1.3.1}Multi-modality state space}{3}{subsection.1.3.1}
\contentsline {subsection}{\numberline {1.3.2}Sparse reward function}{4}{subsection.1.3.2}
\contentsline {subsection}{\numberline {1.3.3}Assumptions on relationship of tasks}{5}{subsection.1.3.3}
\contentsline {section}{\numberline {1.4}Research Questions}{6}{section.1.4}
\contentsline {chapter}{\numberline {2}Related works}{7}{chapter.2}
\contentsline {section}{\numberline {2.1}Policy Gradient Methods}{7}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}Trust Region Policy Optimization}{8}{subsection.2.1.1}
\contentsline {subsection}{\numberline {2.1.2}Kronecker-factored Trust Region}{8}{subsection.2.1.2}
\contentsline {subsection}{\numberline {2.1.3}Proximal Policy Gradient Method}{9}{subsection.2.1.3}
\contentsline {subsection}{\numberline {2.1.4}Other Methods in Deep Reinforcement Learning for Continuous Control}{10}{subsection.2.1.4}
\contentsline {section}{\numberline {2.2}Hierarchical Reinforcement Learning Methods}{10}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Option framework}{11}{subsection.2.2.1}
\contentsline {subsection}{\numberline {2.2.2}Modulated hierarchical controller}{12}{subsection.2.2.2}
\contentsline {subsection}{\numberline {2.2.3}Learning a hierarchical model by meta-learning}{14}{subsection.2.2.3}
\contentsline {subsection}{\numberline {2.2.4}Goal-directed learning method}{15}{subsection.2.2.4}
\contentsline {subsection}{\numberline {2.2.5}Other methods targeting at sparse environments}{16}{subsection.2.2.5}
\contentsline {chapter}{\numberline {3}Methodology}{17}{chapter.3}
\contentsline {section}{\numberline {3.1}Target Environments}{17}{section.3.1}
\contentsline {subsection}{\numberline {3.1.1}Summary of environment design}{17}{subsection.3.1.1}
\contentsline {subsection}{\numberline {3.1.2}Detailed environment specification}{19}{subsection.3.1.2}
\contentsline {section}{\numberline {3.2}Wasserstein Actor Critic Kronecker-factored Trust Region Policy Optimization mehod }{20}{section.3.2}
\contentsline {section}{\numberline {3.3}Efficient Exploration Through Exceptional Advantage Regularization}{23}{section.3.3}
\contentsline {section}{\numberline {3.4}Efficient Exploration Through Robust Concentric Mixture Gaussian Policy}{25}{section.3.4}
\contentsline {section}{\numberline {3.5}Hierarchical reinforcement learning method}{25}{section.3.5}
\contentsline {subsection}{\numberline {3.5.1}Hierarchical reinforcement learning architecture}{25}{subsection.3.5.1}
\contentsline {subsection}{\numberline {3.5.2}Generalized advantage estimation for the decider agent}{26}{subsection.3.5.2}
\contentsline {subsection}{\numberline {3.5.3}Training of the switcher policy}{28}{subsection.3.5.3}
\contentsline {section}{\numberline {3.6}Training of the actuator policies}{28}{section.3.6}
\contentsline {subsection}{\numberline {3.6.1}Effective training of actuator policies}{28}{subsection.3.6.1}
\contentsline {subsection}{\numberline {3.6.2}Domain randomization by cross-sampling initial states}{29}{subsection.3.6.2}
\contentsline {chapter}{\numberline {4}Experiments}{30}{chapter.4}
\contentsline {section}{\numberline {4.1}Experiments on the basic task move0}{30}{section.4.1}
\contentsline {section}{\numberline {4.2}Experiment on the flat reinforcement learning solution to multi-modality tasks}{33}{section.4.2}
\contentsline {subsection}{\numberline {4.2.1}Conventional flat reinforcement learning methods}{33}{subsection.4.2.1}
\contentsline {subsection}{\numberline {4.2.2}Experiment on exceptional advantage regularization}{34}{subsection.4.2.2}
\contentsline {subsection}{\numberline {4.2.3}Experiment on the Robust Concentric Mixture Gaussian Policy}{44}{subsection.4.2.3}
\contentsline {section}{\numberline {4.3}Hierarchical reinforcement learning methods for multi-modality and sparse environments}{45}{section.4.3}
\contentsline {subsection}{\numberline {4.3.1}Training the actuator agents with domain randomization by cross-sampling initial states}{45}{subsection.4.3.1}
\contentsline {subsection}{\numberline {4.3.2}Training the decider agent}{47}{subsection.4.3.2}
\contentsline {subsubsection}{Phase 1: Decision policy training}{48}{section*.1}
\contentsline {subsubsection}{Phase 2: Switcher policy training}{48}{section*.2}
\contentsline {chapter}{\numberline {5}Discussion}{56}{chapter.5}
\contentsline {chapter}{Reference}{58}{chapter.5}
