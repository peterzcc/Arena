\contentsline {chapter}{Title Page}{i}{Doc-Start}
\contentsline {chapter}{Authorization Page}{ii}{Doc-Start}
\contentsline {chapter}{Signature Page}{iii}{Doc-Start}
\contentsline {chapter}{Acknowledgments}{iv}{Doc-Start}
\contentsline {chapter}{Table of Contents}{v}{Doc-Start}
\contentsline {chapter}{List of Figures}{viii}{Doc-Start}
\contentsline {chapter}{List of Tables}{xi}{Doc-Start}
\contentsline {chapter}{Abstract}{xii}{Doc-Start}
\contentsline {chapter}{\numberline {1}Introduction}{1}{chapter.1}
\contentsline {section}{\numberline {1.1}Overview}{1}{section.1.1}
\contentsline {section}{\numberline {1.2}Background}{2}{section.1.2}
\contentsline {subsection}{\numberline {1.2.1}Reinforcement Learning (RL)}{2}{subsection.1.2.1}
\contentsline {section}{\numberline {1.3}Scope of study}{2}{section.1.3}
\contentsline {subsection}{\numberline {1.3.1}Multi-modality state space}{3}{subsection.1.3.1}
\contentsline {subsection}{\numberline {1.3.2}Sparse reward function}{3}{subsection.1.3.2}
\contentsline {subsection}{\numberline {1.3.3}Assumptions on relationship of tasks}{4}{subsection.1.3.3}
\contentsline {section}{\numberline {1.4}Research Questions}{5}{section.1.4}
\contentsline {chapter}{Abbreviations}{1}{Doc-Start}
\contentsline {chapter}{\numberline {2}Related works}{6}{chapter.2}
\contentsline {section}{\numberline {2.1}Policy Gradient Methods}{6}{section.2.1}
\contentsline {subsection}{\numberline {2.1.1}Trust Region Policy Optimization}{7}{subsection.2.1.1}
\contentsline {subsection}{\numberline {2.1.2}Kronecker-factored Trust Region}{7}{subsection.2.1.2}
\contentsline {subsection}{\numberline {2.1.3}Proximal Policy Gradient Method}{8}{subsection.2.1.3}
\contentsline {subsection}{\numberline {2.1.4}Other Methods in Deep Reinforcement Learning for Continuous Control}{9}{subsection.2.1.4}
\contentsline {section}{\numberline {2.2}Hierarchical Reinforcement Learning Methods}{9}{section.2.2}
\contentsline {subsection}{\numberline {2.2.1}Option framework}{10}{subsection.2.2.1}
\contentsline {subsection}{\numberline {2.2.2}Modulated hierarchical controller}{11}{subsection.2.2.2}
\contentsline {subsection}{\numberline {2.2.3}Learning a hierarchical model by meta-learning}{13}{subsection.2.2.3}
\contentsline {subsection}{\numberline {2.2.4}Goal-directed learning method}{14}{subsection.2.2.4}
\contentsline {subsection}{\numberline {2.2.5}Other methods targeting at sparse environments}{15}{subsection.2.2.5}
\contentsline {chapter}{\numberline {3}Methodology}{16}{chapter.3}
\contentsline {section}{\numberline {3.1}Target Environments}{16}{section.3.1}
\contentsline {subsection}{\numberline {3.1.1}Summary of environment design}{16}{subsection.3.1.1}
\contentsline {subsection}{\numberline {3.1.2}Detailed environment specification}{18}{subsection.3.1.2}
\contentsline {section}{\numberline {3.2}Wasserstein Actor Critic Kronecker-factored Trust Region Policy Optimization mehod }{19}{section.3.2}
\contentsline {section}{\numberline {3.3}Efficient Exploration Through Exceptional Advantage Regularization}{22}{section.3.3}
\contentsline {section}{\numberline {3.4}Efficient Exploration Through Robust Concentric Mixture Gaussian Policy}{23}{section.3.4}
\contentsline {section}{\numberline {3.5}Efficient training of actuator policies}{24}{section.3.5}
\contentsline {subsection}{\numberline {3.5.1}Domain randomization by cross-sampling initial states}{24}{subsection.3.5.1}
\contentsline {section}{\numberline {3.6}Hierarchical reinforcement learning architecture}{25}{section.3.6}
\contentsline {section}{\numberline {3.7}Generalized advantage estimation for the decider agent}{26}{section.3.7}
\contentsline {section}{\numberline {3.8}Training of the switcher agent}{27}{section.3.8}
\contentsline {chapter}{\numberline {4}Experiments}{29}{chapter.4}
\contentsline {section}{\numberline {4.1}Experiments on the basic source tasks}{29}{section.4.1}
\contentsline {section}{\numberline {4.2}Experiment on the flat reinforcement learning solution to multi-modality tasks}{32}{section.4.2}
\contentsline {subsection}{\numberline {4.2.1}Discussion on conventional flat reinforcement learning methods}{32}{subsection.4.2.1}
\contentsline {subsection}{\numberline {4.2.2}Experiment on exceptional advantage regularization}{33}{subsection.4.2.2}
\contentsline {subsection}{\numberline {4.2.3}Experiment on the Robust Concentric Mixture Gaussian Policy}{41}{subsection.4.2.3}
\contentsline {section}{\numberline {4.3}Hierarchical reinforcement learning methods for multi-modality and sparse environments}{43}{section.4.3}
\contentsline {subsection}{\numberline {4.3.1}Training the actuator agents with domain randomization by cross-sampling initial states}{44}{subsection.4.3.1}
\contentsline {subsection}{\numberline {4.3.2}Training the decider agent}{47}{subsection.4.3.2}
\contentsline {subsubsection}{Phase 1: Decision policy training}{47}{section*.1}
\contentsline {subsubsection}{Phase 2: Switcher policy training}{47}{section*.2}
\contentsline {chapter}{\numberline {5}Discussion}{54}{chapter.5}
\contentsline {chapter}{Reference}{55}{chapter.5}
